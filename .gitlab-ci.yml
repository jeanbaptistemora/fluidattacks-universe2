image: registry.gitlab.com/fluidattacks/serves:builder

stages:
  - deps
  - analytics
  - terraform-backend
  - tests
  - build
  - mr-check
  - deploy
  - eks-initial-setup
  - postdeploy
  - rotation
  - backup

.vault_vars: &vault_vars
  variables:
    VAULT_ADDR: "https://${VAULT_S3_BUCKET}.com"
    VAULT_HOST: "${VAULT_S3_BUCKET}.com"
    VAULT_PORT: 443
    ROLE_ID: "${SERVES_ROLE_ID}"
    SECRET_ID: "${SERVES_SECRET_ID}"
    VAULTENV_SECRETS_FILE: "${CI_PROJECT_DIR}/env.vars"

.vault_get_token: &vault_get_token |-
  export VAULT_TOKEN=$( \
    vault write -field=token auth/approle/login \
      role_id="${ROLE_ID}" secret_id="${SECRET_ID}" \
  )

.vault_login: &vault_login
  <<: *vault_vars
  before_script:
    - *vault_get_token

.kaniko_config: &kaniko_config
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  before_script:
    - echo '{"auths":{"'"${CI_REGISTRY}"'":{"username":"'"${CI_REGISTRY_USER}"'","password":"'"${CI_REGISTRY_PASSWORD}"'"}}}'
        > /kaniko/.docker/config.json

.kaniko_vault_config: &kaniko_vault_config
  <<: *vault_vars
  <<: *kaniko_config
  before_script:
    - wget -O vault.zip https://releases.hashicorp.com/vault/0.11.6/vault_0.11.6_linux_amd64.zip;
        unzip vault.zip;
        rm vault.zip;
        mv vault /busybox/
    - *vault_get_token
    - echo '{"auths":{"'"${CI_REGISTRY}"'":{"username":"'"${CI_REGISTRY_USER}"'","password":"'"${CI_REGISTRY_PASSWORD}"'"}}}'
        > /kaniko/.docker/config.json

.kaniko_build_template: &kaniko_build_template
  script:
    - export ANSIBLE_VAULT=$(vault read -field=ansible_vault secret/serves)
    - export APP=${CI_JOB_NAME#*_}
    - /kaniko/executor
        --build-arg ANSIBLE_VAULT="${ANSIBLE_VAULT}"
        --cleanup
        --context "${CI_PROJECT_DIR}/containers/${APP}/"
        --dockerfile "${CI_PROJECT_DIR}/containers/${APP}/Dockerfile"
        --destination "${CI_REGISTRY_IMAGE}/${APP}:${CI_COMMIT_REF_SLUG}"
        --single-snapshot

.only_schedules: &only_schedules
  only:
    refs:
      - schedules

.except_analytics: &except_analytics
  except:
    variables:
      - $FLAG_REFRESH_TOKENS
      - $FLAG_CONT_MS_SQL_SERVER
      - $FLAG_SYNC_CURRENCIES
      - $FLAG_SYNC_FORMSTACK
      - $FLAG_SYNC_AWSDYNAMODB
      - $FLAG_SYNC_TIMEDOCTOR
      - $FLAG_SYNC_GIT
      - $FLAG_SYNC_MANDRILL
      - $FLAG_SYNC_INFRASTRUCTURE
      - $FLAG_SYNC_MS_SQL_SERVER_ERP
      - $FLAG_SYNC_RINGCENTRAL
      - $FLAG_SYNC_SALESFORCE
      - $FLAG_SYNC_INTERCOM
      - $FLAG_SYNC_GITLAB
      - $FLAG_SYNC_CONTINUOUS
      - $FLAG_SYNC_ZOHO

build_builder:
  stage: deps
  <<: *kaniko_config
  script:
    - /kaniko/executor
        --context "${CI_PROJECT_DIR}/containers/builder/"
        --dockerfile "${CI_PROJECT_DIR}/containers/builder/Dockerfile"
        --destination "${CI_REGISTRY_IMAGE}:builder"
  <<: *only_schedules
  <<: *except_analytics

test_code:
  stage: tests
  script:
    - ./check-changed.sh
  except:
    - master

commitlint:
  stage: tests
  image: starefossen/ruby-node:2-10
  before_script:
    - npm install --unsafe-perm
  script:
    - ./ci-scripts/commitlint-checks.sh
  except:
    - master
    - schedules

test_terraform:
  stage: tests
  <<: *vault_login
  script:
    - vaultenv infrastructure/terraform.sh
  after_script:
    - rm infrastructure/*.xml
  except:
    refs:
      - master

build_exams:
  stage: build
  <<: *kaniko_vault_config
  <<: *kaniko_build_template
  <<: *except_analytics

build_vpn:
  stage: build
  <<: *kaniko_vault_config
  <<: *kaniko_build_template
  <<: *except_analytics

danger:
  stage: mr-check
  image: fluidattacks/danger-ruby
  only:
    - merge_requests
  variables:
    DANGER_GITLAB_API_TOKEN: $DANGER_TOKEN
  before_script:
    - export CI_MERGE_REQUEST_ID=$(git ls-remote -q origin merge-requests\*\head
      | grep ${CI_COMMIT_SHA}
      | sed 's/.*refs\/merge-requests\/\([0-9]*\)\/head/\1/g')
    - npm install --unsafe-perm
  script:
    - danger --verbose --fail-on-errors=true

deploy_infra:
  stage: deploy
  <<: *vault_login
  script:
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/kubeconfig
        $HOME/.kube/config || echo "No Kubernetes configuration file found"
    - sed -i 's/plan/apply\ -auto-approve/g' infrastructure/terraform.sh
    - vaultenv infrastructure/terraform.sh deployment
  after_script:
    - aws s3 cp
        $HOME/.kube/config
        s3://$FS_S3_BUCKET_NAME/terraform/kubeconfig
    - rm infrastructure/*.xml
  only:
    - master
  except:
    - schedules

new_version_mail:
  stage: postdeploy
  <<: *vault_login
  script:
    - curl -Lo mail.py https://gitlab.com/fluidattacks/default/raw/master/shared-scripts/mail.py
    - "echo \"send_mail('new_version',
        MANDRILL_EMAIL_TO,
        context={'project': PROJECT, 'project_url': '${CI_PROJECT_URL}',
          'version': _get_version_date(), 'message': _get_message()},
        tags=['general'])\" >> mail.py"
    - vaultenv -- python3 mail.py
  only:
    - master
  except:
    - schedules

fluidasserts_post:
  stage: postdeploy
  image: fluidattacks/asserts
  script:
    - asserts asserts/exploit.py
  only:
    - master
  except:
    - schedules

change_keys:
  stage: rotation
  <<: *vault_login
  script:
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/kubeconfig
        $HOME/.kube/config
    - cd infrastructure
    - source vault-wrapper.sh
    - vault_generate_aws_keys integrates-cloudwatch
    - vault_generate_aws_keys integrates-dynamodb
    - vault_generate_aws_keys integrates-s3
    - vault_generate_aws_keys integrates-terraform
    - vault_generate_aws_keys web-s3
    - NEW_JWT_SECRET=$(head -c 32 /dev/urandom | base64)
    - vault_update_variables integrates/development jwt_secret $NEW_JWT_SECRET
    - vault_update_variables integrates/production jwt_secret $NEW_JWT_SECRET
    - export FORMSTACK_EMAIL="$(vault read -field=formstack_email
        secret/serves)"
    - export FORMSTACK_PASS="$(vault read -field=formstack_pass
        secret/serves)"
    - FORMSTACK_TOKENS=`./rotate_fs_keys.py`
    - vault_update_variables integrates/development
        formstack_tokens "$FORMSTACK_TOKENS"
    - vault_update_variables integrates/production
        formstack_tokens "$FORMSTACK_TOKENS"
    - export INTEGRATES_VAULT_TOKEN=$(curl --request POST
        --data '{"role_id":"'"$INTEGRATES_PROD_ROLE_ID"'","secret_id":"'"$INTEGRATES_PROD_SECRET_ID"'"}'
        "https://$VAULT_S3_BUCKET.com/v1/auth/approle/login" |
        jq -r '.auth.client_token')
    - sed -i 's/$FI_VAULT_HOST/'"$(echo -n $VAULT_HOST | base64)"'/;
        s/$FI_VAULT_TOKEN/'"$(echo -n $INTEGRATES_VAULT_TOKEN | base64)"'/'
        eks/manifests/deployments/integrates-app.yaml
    - sed -i 's/$DATE/'"$(date)"'/' eks/manifests/deployments/*.yaml
    - kubectl apply -f eks/manifests/deployments/integrates-app.yaml
    - kubectl rollout status deploy/integrates-app --timeout=5m ||
        { kubectl rollout undo deploy/integrates-app && exit 1; }
    - python3 -c "from rotate_fs_keys import delete_formstack_tokens;
        delete_formstack_tokens()"
  <<: *only_schedules
  <<: *except_analytics
  when: always

vault_backup:
  stage: backup
  script:
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/kubeconfig
        $HOME/.kube/config
    - cd infrastructure/eks/manifests/vault/
    - kubectl apply -f backup-operator.yaml
    - kubectl rollout status deploy/vault-etcd-operator-etcd-backup-operator
    - envsubst < credentials > creds
        && mv creds credentials
    - envsubst < config > conf
        && mv conf config
    - kubectl create secret generic aws
        --from-file=credentials
        --from-file=config
    - export DATE=$(date +%Y-%m-%d)
    - envsubst < backup.yaml > backup
        && mv backup backup.yaml
    - kubectl apply -f backup.yaml
    - while ! aws s3 ls s3://$VAULT_S3_BUCKET | grep backup-$(date +%Y-%m-%d);
        do sleep 2;
      done
    - kubectl delete -f backup.yaml
    - kubectl delete secret aws
    - kubectl delete -f backup-operator.yaml
    - rm credentials
  <<: *only_schedules
  <<: *except_analytics

formstack_backup:
  stage: backup
  <<: *vault_login
  script:
    - pip3 install boto3
    - export FORMSTACK_EMAIL="$(vault read -field=formstack_email
        secret/serves)"
    - export FORMSTACK_PASS="$(vault read -field=formstack_pass
        secret/serves)"
    - ./infrastructure/fs_backup.py
  <<: *only_schedules
  <<: *except_analytics

sync_currencies:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/tap_currencyconverterapi
        analytics/singer/target_redshift
    - tap-currencyconverterapi > /currencies.singer
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - cat /currencies.singer |
        target-redshift --auth /target_secret.json --drop-schema --schema-name "currencies"
    - rm -fr /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_CURRENCIES

sync_formstack:
  stage: analytics
  <<: *vault_login
  script:
    - mkdir /logs
    - analytics/set-aws-cli.sh
    - pip3 install
        analytics/singer/tap_formstack
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_formstack secret/serves)" > /tap_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - tap-formstack --auth /tap_secret.json --conf analytics/conf/formstack.json > .singer
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "formstack"
    - aws s3 cp /logs s3://fluidanalytics/formstack --recursive
  after_script:
    - rm -fr /tap_secret.json /target_secret.json ~/.aws
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_FORMSTACK

sync_awsdynamodb:
  stage: analytics
  <<: *vault_login
  script:
    - mkdir /logs
    - pip3 install
        analytics/singer/tap_awsdynamodb
        analytics/singer/target_redshift
    - echo '{ "AWS_ACCESS_KEY_ID":"'$(vault read
        -field=aws_dynamodb_access_key secret/serves)'",
              "AWS_SECRET_ACCESS_KEY":"'$(vault read
        -field=aws_dynamodb_secret_key secret/serves)'",
              "AWS_DEFAULT_REGION":"'$(vault read
        -field=aws_dynamodb_default_region secret/serves)'"}' > /tap_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - tap-awsdynamodb --auth /tap_secret.json --conf analytics/conf/awsdynamodb.json > .singer
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "dynamodb"
  after_script:
    - rm -fr /tap_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_AWSDYNAMODB

sync_timedoctor:
  stage: analytics
  <<: *vault_login
  script:
    - mkdir /logs
    - pip3 install
        boto3
        analytics/singer/tap_timedoctor
        analytics/singer/target_redshift
    - echo '{ "AWS_ACCESS_KEY_ID":"'$(vault read
        -field=aws_s3_access_key secret/serves)'",
              "AWS_SECRET_ACCESS_KEY":"'$(vault read
        -field=aws_s3_secret_key secret/serves)'",
              "AWS_DEFAULT_REGION":"'$(vault read
        -field=aws_s3_default_region secret/serves)'"}' > /s3_auth.json
    - echo "$(vault read -field=analytics_s3_cache_timedoctor secret/serves)" > /s3_files.json
    - echo "$(vault read -field=analytics_auth_timedoctor secret/serves)" > /tap_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - python3 analytics/download_from_aws_sss.py -auth /s3_auth.json -conf /s3_files.json
    - cat timedoctor.worklogs.2013-01-01.2018-12-31.singer > timedoctor.singer
    - cat timedoctor.computer_activity.2018-01-01.2018-12-31.singer >> timedoctor.singer
    - tap-timedoctor --auth /tap_secret.json >> timedoctor.singer
    - cat timedoctor.singer |
        target-redshift --auth /target_secret.json --drop-schema --schema-name "timedoctor"
  after_script:
    - rm -fr /s3_auth.json /s3_files.json /tap_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_TIMEDOCTOR

sync_git:
  stage: analytics
  <<: *vault_login
  script:
    - analytics/git/set_dependencies.sh
    - python3 analytics/git/clone_us.py 2>&1 | aws s3 cp - s3://fluidanalytics/clone_us.log
    - python3 analytics/git/clone_them.py 2>&1 | tee clone_them.log | aws s3 cp - s3://fluidanalytics/clone_them.log
    - python3 analytics/git/generate_stats.py || true
    - python3 analytics/git/generate_config.py 2>&1 | aws s3 cp - s3://fluidanalytics/generate_config.log
    - analytics/git/sync_forked.sh
    - analytics/git/mirror.py 2>&1 | aws s3 cp - s3://fluidanalytics/mirrors.log
    - analytics/git/taint_all.sh
  after_script:
    - rm -fr ~/.aws
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_GIT

sync_mandrill:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/streamer_mandrill
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_mandrill secret/serves)" > /stream_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - streamer-mandrill --auth /stream_secret.json > mandrill.jsonstream
    - cat mandrill.jsonstream | tap-json --date-formats "%Y-%m-%d %H:%M:%S,%Y-%m-%d %H:%M:%S.%f" > mandrill.singer
    - cat mandrill.singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "mandrill"
  after_script:
    - rm -rf /stream_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_MANDRILL

sync_infrastructure:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/streamer_infrastructure
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_infra secret/serves)" > /stream_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - streamer-infrastructure --auth /stream_secret.json > infra.jsonstream
    - cat infra.jsonstream | tap-json > infra.singer
    - cat infra.singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "infrastructure"
  after_script:
    - rm -rf /stream_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_INFRASTRUCTURE

build_ms_sql_server:
  stage: deps
  <<: *kaniko_config
  script:
    - export CONT="ms_sql_server"
    - export CONT_DIR="${CI_PROJECT_DIR}/analytics/containers/${CONT}"
    - cp env.vars "${CONT_DIR}"
    - /kaniko/executor
        --cleanup
        --context "${CONT_DIR}"
        --dockerfile "${CONT_DIR}/Dockerfile"
        --destination "${CI_REGISTRY_IMAGE}:${CONT}"
  <<: *only_schedules
  only:
    variables:
      - $FLAG_CONT_MS_SQL_SERVER

sync_ms_sql_server_erp:
  image: registry.gitlab.com/fluidattacks/serves:ms_sql_server
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        pyodbc
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_ms_sql_server_erp secret/serves)" > /convert_secret.json
    - mkdir csv
    - analytics/singer/converter_mssqlserver_csv.py --auth /convert_secret.json --output-dir csv
    - for csv_path in csv/*.csv; do
        echo "$csv_path";
        analytics/singer/streamer_csv.py "$csv_path" >> .jsonstream;
      done
    - cat .jsonstream | tap-json > .singer
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "erp"
  after_script:
    - rm -rf /convert_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_MS_SQL_SERVER_ERP

sync_ringcentral:
  image: registry.gitlab.com/fluidattacks/serves:ms_sql_server
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/streamer_ringcentral
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_ringcentral secret/serves)" > /stream_secret.json
    - echo "$(vault read -field=analytics_auth_redshift    secret/serves)" > /target_secret.json
    - streamer-ringcentral --auth /stream_secret.json --sync-user --sync-calls > .jsonstream
    - cat .jsonstream | tap-json > .singer
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "ringcentral"
  after_script:
    - rm -rf /stream_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_RINGCENTRAL

sync_salesforce:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        boto3
        tap_salesforce
        analytics/singer/target_redshift
    - echo '{ "AWS_ACCESS_KEY_ID":"'$(vault read -field=aws_s3_access_key secret/serves)'",
              "AWS_SECRET_ACCESS_KEY":"'$(vault read -field=aws_s3_secret_key secret/serves)'",
              "AWS_DEFAULT_REGION":"'$(vault read -field=aws_s3_default_region secret/serves)'"}' > /s3_auth.json
    - echo "$(vault read -field=analytics_s3_conf_salesforce secret/serves)" > /s3_files.json
    - echo "$(vault read -field=analytics_auth_salesforce secret/serves)" > /tap_secret.json
    - echo "$(vault read -field=analytics_auth_redshift   secret/serves)" > /target_secret.json
    - analytics/download_from_aws_sss.py -auth /s3_auth.json -conf /s3_files.json
    - tap-salesforce --config /tap_secret.json --properties /tap_config.json > .singer
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "salesforce"
  after_script:
    - rm -rf /s3_auth.json /s3_files.json /tap_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_SALESFORCE

sync_intercom:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/streamer_intercom
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_intercom secret/serves)" > /stream_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - streamer-intercom --auth /stream_secret.json > .jsonstream
    - cat .jsonstream | tap-json --enable-timestamps > .singer
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "intercom"
  after_script:
    - rm -rf /stream_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_INTERCOM

sync_gitlab:
  stage: analytics
  <<: *vault_login
  script:
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - export GITLAB_PASS="$(vault read -field=analytics_gitlab_token secret/serves)"
    - GITLAB_PROJECTS=(
        "autonomicmind/default" "autonomicmind/training"
        "fluidattacks/bwapp" "fluidattacks/continuous" "fluidattacks/default"
        "fluidattacks/serves" "fluidattacks/asserts" "fluidattacks/default"
        "fluidattacks/integrates" "fluidattacks/web" "fluidattacks/writeups")
    - for project in ${GITLAB_PROJECTS[*]}; do
        ./analytics/singer/streamer_gitlab.py "${project}" >> .jsonstream;
      done
    - pip3 install
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - cat .jsonstream | tap-json > .singer
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "gitlab-ci"
  after_script:
    - unset GITLAB_PASS
    - rm -rf /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_GITLAB

sync_continuous:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - cd analytics/continuous
    - GITLAB_USER=$(vault read -field=analytics_gitlab_user secret/serves)
    - GITLAB_PASS=$(vault read -field=analytics_gitlab_token secret/serves)
    - vault read -field=analytics_auth_redshift secret/serves > /target_secret.json
    - git clone --depth 1 --single-branch
        https://${GITLAB_USER}:${GITLAB_PASS}@gitlab.com/fluidattacks/continuous.git
    - ./streamer_toe.py > .jsonstream
    - cat .jsonstream | tap-json > .singer
    - cat .singer | target-redshift --auth /target_secret.json
                                    --drop-schema
                                    --schema-name continuous_toe
  after_script:
    - unset GITLAB_USER GITLAB_PASS
    - rm -rf /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_CONTINUOUS

analytics_refresh_token_timedoctor:
  stage: analytics
  <<: *vault_login
  script:
    - ./analytics/auth_helper.py --timedoctor-refresh
  <<: *only_schedules
  only:
    variables:
      - $FLAG_REFRESH_TOKENS

sync_zoho:
  stage: analytics
  <<: *vault_vars
  before_script:
    - *vault_get_token
    - set -ETeumo pipefail
  script:
    - pip3 install --upgrade requests
    - email="$(vault read -field=analytics_zoho_email secret/serves)"
    - token="$(vault read -field=analytics_zoho_token secret/serves)"
    - space="$(vault read -field=analytics_zoho_space secret/serves)"
    - vault read -field=analytics_auth_redshift secret/serves > secret.json
    - vault read -field=analytics_zoho_tables secret/serves | while read -r table;
        do
          ./analytics/singer/converter_zoho_csv.py
            --email "${email}" --token "${token}"
            --space "${space}" --table "${table}" --target "${table}" &&
          ./analytics/singer/streamer_csv.py "${table}" >> .jsonstream;
        done
    - pip3 install
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - cat .jsonstream | tap-json --date-formats '%Y-%m-%d %H:%M:%S' > .singer
    - cat .singer | target-redshift --schema-name "zoho"
                                    --auth secret.json
                                    --drop-schema
  after_script:
    - rm -rf secret.json
    - unset email token space
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_ZOHO

eks-initial-setup:
  stage: eks-initial-setup
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - . toolbox/others.sh && vault_login
    - vaultenv ./ci-scripts/jobs/eks-initial-setup.sh
  only:
    - master
  except:
    - schedules

eks-terraform-apply:
  stage: deploy
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/eks-terraform-apply.sh
  only:
    - master
  except:
    - schedules

eks-terraform-lint:
  stage: tests
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/eks-terraform-lint.sh
  except:
    - schedules

eks-terraform-plan:
  stage: tests
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/eks-terraform-plan.sh
  except:
    - schedules

terraform-states-bucket:
  stage: terraform-backend
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/terraform-states-bucket.sh
  only:
    - master
  except:
    - schedules

sops-terraform-lint:
  stage: tests
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/sops-terraform-lint.sh
  except:
    - schedules

sops-terraform-plan:
  stage: tests
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/sops-terraform-plan.sh
  except:
    - schedules

sops-terraform-apply:
  stage: deploy
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/sops-terraform-apply.sh
  only:
    - master
  except:
    - schedules

aws-sso-terraform-lint:
  stage: tests
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/aws-sso-terraform-lint.sh
  except:
    - schedules

aws-sso-terraform-plan:
  stage: tests
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/aws-sso-terraform-plan.sh
  except:
    - schedules

aws-sso-terraform-apply:
  stage: deploy
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/aws-sso-terraform-apply.sh
  only:
    - master
  except:
    - schedules

break-build-terraform-apply:
  stage: deploy
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/break-build-terraform-apply.sh
  only:
    - master
  except:
    - schedules

break-build-terraform-lint:
  stage: tests
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/break-build-terraform-lint.sh
  except:
    - schedules

break-build-terraform-plan:
  stage: tests
  image: registry.gitlab.com/fluidattacks/serves:builder
  script:
    - ./ci-scripts/jobs/break-build-terraform-plan.sh
  except:
    - schedules
