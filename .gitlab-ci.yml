image: registry.gitlab.com/fluidsignal/serves:builder

stages:
  - deps
  - analytics
  - tests
  - build
  - mr-check
  - deploy
  - postdeploy
  - rotation
  - backup

.vault_vars: &vault_vars
  variables:
    VAULT_HOST: "${VAULT_S3_BUCKET}.com"
    VAULT_PORT: 443
    VAULT_API_URL: "https://${VAULT_S3_BUCKET}.com/v1/auth/approle/login"
    ROLE_ID: "${SERVES_ROLE_ID}"
    SECRET_ID: "${SERVES_SECRET_ID}"
    VAULTENV_SECRETS_FILE: "${CI_PROJECT_DIR}/env.vars"

.vault_login: &vault_login
  <<: *vault_vars
  before_script:
    - export VAULT_TOKEN=$(curl
        --request POST
        --data '{"role_id":"'"${ROLE_ID}"'","secret_id":"'"${SECRET_ID}"'"}'
        "${VAULT_API_URL}" | jq -r '.auth.client_token')

.kaniko_config: &kaniko_config
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  before_script:
    - echo '{"auths":{"'"${CI_REGISTRY}"'":{"username":"'"${CI_REGISTRY_USER}"'","password":"'"${CI_REGISTRY_PASSWORD}"'"}}}'
        > /kaniko/.docker/config.json

.kaniko_vault_config: &kaniko_vault_config
  <<: *vault_vars
  <<: *kaniko_config

.kaniko_build_template: &kaniko_build_template
  script:
    - export APP=${CI_JOB_NAME#*_}
    - cp env.vars "containers/${APP}/"
    - /kaniko/executor
        --build-arg VAULT_HOST="${VAULT_HOST}"
        --build-arg VAULT_PORT="${VAULT_PORT}"
        --build-arg VAULT_API_URL="${VAULT_API_URL}"
        --build-arg ROLE_ID="${ROLE_ID}"
        --build-arg SECRET_ID="${SECRET_ID}"
        --cleanup
        --context "${CI_PROJECT_DIR}/containers/${APP}/"
        --dockerfile "${CI_PROJECT_DIR}/containers/${APP}/Dockerfile"
        --destination "${CI_REGISTRY_IMAGE}/${APP}:${CI_COMMIT_REF_SLUG}"

.only_schedules: &only_schedules
  only:
    refs:
      - schedules

.except_analytics: &except_analytics
  except:
    variables:
      - $FLAG_CONT_MS_SQL_SERVER
      - $FLAG_SYNC_CURRENCIES
      - $FLAG_SYNC_FORMSTACK
      - $FLAG_SYNC_AWSDYNAMODB
      - $FLAG_SYNC_TIMEDOCTOR
      - $FLAG_SYNC_GIT
      - $FLAG_SYNC_MANDRILL
      - $FLAG_SYNC_INFRASTRUCTURE
      - $FLAG_SYNC_MS_SQL_SERVER_ERP
      - $FLAG_SYNC_SALESFORCE

build_base:
  stage: deps
  <<: *kaniko_config
  script:
    - /kaniko/executor
        --context "${CI_PROJECT_DIR}/containers/base/"
        --dockerfile "${CI_PROJECT_DIR}/containers/base/Dockerfile"
        --destination "${CI_REGISTRY_IMAGE}:base"
  <<: *only_schedules
  <<: *except_analytics

build_builder:
  stage: deps
  <<: *kaniko_config
  script:
    - /kaniko/executor
        --context "${CI_PROJECT_DIR}/containers/builder/"
        --dockerfile "${CI_PROJECT_DIR}/containers/builder/Dockerfile"
        --destination "${CI_REGISTRY_IMAGE}:builder"
  <<: *only_schedules
  <<: *except_analytics

test_code:
  stage: tests
  script:
    - ./check-changed.sh
  except:
    - master

commitlint:
  stage: tests
  image: starefossen/ruby-node:2-10
  before_script:
    - npm install --unsafe-perm
  script:
    - ./ci-scripts/commitlint-checks.sh
  except:
    - master
    - schedules

test_terraform:
  stage: tests
  <<: *vault_login
  script:
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/terraform.tfstate
        infrastructure/terraform.tfstate ||
        echo "No previous state for infrastructure found"
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/dns/terraform.tfstate
        infrastructure/dns/terraform.tfstate ||
        echo "No previous state for DNS found"
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/staging/terraform.tfstate
        infrastructure/staging/terraform.tfstate ||
        echo "No previous state for staging found"
    - vaultenv infrastructure/terraform.sh
  after_script:
    - rm infrastructure/*.xml
  except:
    - master

build_exams:
  stage: build
  <<: *kaniko_vault_config
  <<: *kaniko_build_template
  only:
    changes:
      - containers/exams/**/*
  <<: *except_analytics

build_vpn:
  stage: build
  <<: *kaniko_vault_config
  <<: *kaniko_build_template
  only:
    changes:
      - containers/vpn/**/*
  <<: *except_analytics

mr-test:
  stage: mr-check
  only:
    - merge_requests
  variables:
    GIT_STRATEGY: clone
  script:
    ./ci-scripts/check-branch.sh

danger:
  stage: mr-check
  image: starefossen/ruby-node:2-10
  variables:
    DANGER_GITLAB_API_TOKEN: ${DANGER_TOKEN}
    DANGER_GITLAB_HOST: "gitlab.com"
    DANGER_GITLAB_API_BASE_URL: "https://gitlab.com/api/v4"
  before_script:
    - export CI_MERGE_REQUEST_ID=$(git ls-remote -q origin merge-requests\*\head
      | grep ${CI_COMMIT_SHA}
      | sed 's/.*refs\/merge-requests\/\([0-9]*\)\/head/\1/g')
    - npm install --unsafe-perm
    - bundle install
  script:
    - bundle exec danger --verbose --fail-on-errors=true
  only:
    - merge_requests

deploy_infra:
  stage: deploy
  <<: *vault_login
  script:
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/terraform.tfstate
        infrastructure/terraform.tfstate ||
        (echo "No previous state found" && NEW_DEPLOY=true)
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/dns/terraform.tfstate
        infrastructure/dns/terraform.tfstate ||
        echo "No previous state for DNS found"
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/staging/terraform.tfstate
        infrastructure/staging/terraform.tfstate ||
        echo "No previous state for staging found"
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/kubeconfig
        $HOME/.kube/config || echo "No Kubernetes configuration file found"
    - sed -i 's/plan/apply\ -auto-approve/g' infrastructure/terraform.sh
    - vaultenv infrastructure/terraform.sh deployment
  after_script:
    - aws s3 cp
        infrastructure/terraform.tfstate
        s3://$FS_S3_BUCKET_NAME/terraform/terraform.tfstate
    - aws s3 cp
        infrastructure/dns/terraform.tfstate
        s3://$FS_S3_BUCKET_NAME/terraform/dns/terraform.tfstate
    - aws s3 cp
        infrastructure/staging/terraform.tfstate
        s3://$FS_S3_BUCKET_NAME/terraform/staging/terraform.tfstate ||
        echo "No previous state for staging found"
    - aws s3 cp
        $HOME/.kube/config
        s3://$FS_S3_BUCKET_NAME/terraform/kubeconfig
    - aws s3 cp
        "$HOME/vault-ca.crt"
        s3://$VAULT_S3_BUCKET/vault-ca.crt
    - rm infrastructure/*.xml
  only:
    - master
  except:
    - schedules

fluidasserts_post:
  stage: postdeploy
  image: fluidattacks/asserts
  script:
    - asserts asserts/exploit.py
  only:
    - master
  except:
    - schedules

change_keys:
  stage: rotation
  <<: *vault_login
  script:
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/kubeconfig
        $HOME/.kube/config
    - cd infrastructure
    - source vault-wrapper.sh
    - vault_generate_aws_keys integrates-cloudwatch
    - vault_generate_aws_keys integrates-dynamodb
    - vault_generate_aws_keys integrates-s3
    - vault_generate_aws_keys web-s3
    - NEW_JWT_SECRET=$(head -c 32 /dev/urandom | base64)
    - vault_update_variables integrates/development jwt_secret $NEW_JWT_SECRET
    - vault_update_variables integrates/production jwt_secret $NEW_JWT_SECRET
    - export FORMSTACK_EMAIL="$(vault read -field=formstack_email
        secret/serves)"
    - export FORMSTACK_PASS="$(vault read -field=formstack_pass
        secret/serves)"
    - FORMSTACK_TOKENS=`./rotate_fs_keys.py`
    - vault_update_variables integrates/development
        formstack_tokens "$FORMSTACK_TOKENS"
    - vault_update_variables integrates/production
        formstack_tokens "$FORMSTACK_TOKENS"
    - export INTEGRATES_VAULT_TOKEN=$(curl --request POST
        --data '{"role_id":"'"$INTEGRATES_PROD_ROLE_ID"'","secret_id":"'"$INTEGRATES_PROD_SECRET_ID"'"}'
        "https://$VAULT_S3_BUCKET.com/v1/auth/approle/login" |
        jq -r '.auth.client_token')
    - sed -i 's/$FI_VAULT_HOST/'"$(echo -n $VAULT_HOST | base64)"'/;
        s/$FI_VAULT_TOKEN/'"$(echo -n $INTEGRATES_VAULT_TOKEN | base64)"'/'
        eks/manifests/deployments/integrates.yaml
    - sed -i 's/$DATE/'"$(date)"'/' eks/manifests/deployments/*.yaml
    - kubectl apply -f eks/manifests/deployments/integrates.yaml
    - kubectl rollout status deploy/integrates -w
    - python3 -c "from rotate_fs_keys import delete_formstack_tokens;
        delete_formstack_tokens()"
  <<: *only_schedules
  <<: *except_analytics

vault_backup:
  stage: backup
  script:
    - aws s3 cp
        s3://$FS_S3_BUCKET_NAME/terraform/kubeconfig
        $HOME/.kube/config
    - cd infrastructure/eks/manifests/vault/
    - kubectl apply -f backup-operator.yaml
    - kubectl rollout status deploy/vault-etcd-operator-etcd-backup-operator
    - envsubst < credentials > creds
        && mv creds credentials
    - envsubst < config > conf
        && mv conf config
    - kubectl create secret generic aws
        --from-file=credentials
        --from-file=config
    - export DATE=$(date +%Y-%m-%d)
    - envsubst < backup.yaml > backup
        && mv backup backup.yaml
    - kubectl apply -f backup.yaml
    - while ! aws s3 ls s3://$VAULT_S3_BUCKET | grep backup-$(date +%Y-%m-%d);
        do sleep 2;
      done
    - kubectl delete -f backup.yaml
    - kubectl delete secret aws
    - kubectl delete -f backup-operator.yaml
    - rm credentials
  <<: *only_schedules
  <<: *except_analytics

formstack_backup:
  stage: backup
  <<: *vault_login
  script:
    - pip3 install boto3
    - export FORMSTACK_EMAIL="$(vault read -field=formstack_email
        secret/serves)"
    - export FORMSTACK_PASS="$(vault read -field=formstack_pass
        secret/serves)"
    - ./infrastructure/fs_backup.py
  <<: *only_schedules
  <<: *except_analytics

sync_currencies:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/tap_currencyconverterapi
        analytics/singer/target_redshift
    - tap-currencyconverterapi > /currencies.singer
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - cat /currencies.singer |
        target-redshift --auth /target_secret.json --drop-schema --schema-name "currencies"
    - rm -fr /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_CURRENCIES

sync_formstack:
  stage: analytics
  <<: *vault_login
  script:
    - mkdir /logs
    - pip3 install
        analytics/singer/tap_formstack
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_formstack secret/serves)" > /tap_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - tap-formstack --auth /tap_secret.json --conf analytics/conf/formstack.json |
      target-redshift --auth /target_secret.json --drop-schema --schema-name "formstack"
    - rm -fr /tap_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_FORMSTACK

sync_awsdynamodb:
  stage: analytics
  <<: *vault_login
  script:
    - mkdir /logs
    - pip3 install
        analytics/singer/tap_awsdynamodb
        analytics/singer/target_redshift
    - echo '{ "AWS_ACCESS_KEY_ID":"'$(vault read
        -field=aws_dynamodb_access_key secret/serves)'",
              "AWS_SECRET_ACCESS_KEY":"'$(vault read
        -field=aws_dynamodb_secret_key secret/serves)'",
              "AWS_DEFAULT_REGION":"'$(vault read
        -field=aws_dynamodb_default_region secret/serves)'"}' > /tap_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - tap-awsdynamodb --auth /tap_secret.json --conf analytics/conf/awsdynamodb.json |
      target-redshift --auth /target_secret.json --drop-schema --schema-name "dynamodb"
    - rm -fr /tap_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_AWSDYNAMODB

sync_timedoctor:
  stage: analytics
  <<: *vault_login
  script:
    - mkdir /logs
    - pip3 install
        boto3
        analytics/singer/tap_timedoctor
        analytics/singer/target_redshift
    - echo '{ "AWS_ACCESS_KEY_ID":"'$(vault read
        -field=aws_s3_access_key secret/serves)'",
              "AWS_SECRET_ACCESS_KEY":"'$(vault read
        -field=aws_s3_secret_key secret/serves)'",
              "AWS_DEFAULT_REGION":"'$(vault read
        -field=aws_s3_default_region secret/serves)'"}' > /s3_auth.json
    - echo "$(vault read -field=analytics_s3_cache_timedoctor secret/serves)" > /s3_files.json
    - echo "$(vault read -field=analytics_auth_timedoctor secret/serves)" > /tap_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - python3 analytics/download_from_aws_sss.py -auth /s3_auth.json -conf /s3_files.json
    - cat timedoctor.worklogs.2013-01-01.2018-12-31.singer > timedoctor.singer
    - cat timedoctor.computer_activity.2018-01-01.2018-12-31.singer >> timedoctor.singer
    - tap-timedoctor --auth /tap_secret.json >> timedoctor.singer
    - cat timedoctor.singer |
        target-redshift --auth /target_secret.json --drop-schema --schema-name "timedoctor"
    - rm -fr /s3_auth.json /s3_files.json /tap_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_TIMEDOCTOR

sync_git:
  stage: analytics
  <<: *vault_login
  script:
    - analytics/git/set_dependencies.sh
    - python3 analytics/git/clone_us.py 2>&1 | aws s3 cp - s3://fluidanalytics/clone_us.log
    - python3 analytics/git/clone_them.py 2>&1 | tee clone_them.log | aws s3 cp - s3://fluidanalytics/clone_them.log
    - python3 analytics/git/clone_stats.py || true
    - python3 analytics/git/generate_config.py 2>&1 | aws s3 cp - s3://fluidanalytics/generate_config.log
    - analytics/git/sync_forked.sh
    - analytics/git/mirror.py 2>&1 | aws s3 cp - s3://fluidanalytics/mirrors.log
    - analytics/git/taint_all.sh
    - rm -fr ~/.aws/{credentials, config}
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_GIT

sync_mandrill:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/streamer_mandrill
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_mandrill secret/serves)" > /stream_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - streamer-mandrill --auth /stream_secret.json > mandrill.jsonstream
    - cat mandrill.jsonstream | tap-json --date-formats "%Y-%m-%d %H:%M:%S,%Y-%m-%d %H:%M:%S.%f" > mandrill.singer
    - cat mandrill.singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "mandrill"
    - rm -rf /stream_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_MANDRILL

sync_infrastructure:
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        analytics/singer/streamer_infrastructure
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_infra secret/serves)" > /stream_secret.json
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - streamer-infrastructure --auth /stream_secret.json > infra.jsonstream
    - cat infra.jsonstream | tap-json > infra.singer
    - cat infra.singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "infrastructure"
    - rm -rf /stream_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_INFRASTRUCTURE

build_ms_sql_server:
  stage: deps
  <<: *kaniko_config
  script:
    - export CONT="ms_sql_server"
    - export CONT_DIR="${CI_PROJECT_DIR}/analytics/containers/${CONT}"
    - cp env.vars "${CONT_DIR}"
    - /kaniko/executor
        --cleanup
        --context "${CONT_DIR}"
        --dockerfile "${CONT_DIR}/Dockerfile"
        --destination "${CI_REGISTRY_IMAGE}:${CONT}"
  <<: *only_schedules
  only:
    variables:
      - $FLAG_CONT_MS_SQL_SERVER

sync_ms_sql_server_erp:
  image: registry.gitlab.com/fluidsignal/serves:ms_sql_server
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        pyodbc
        analytics/singer/tap_json
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_ms_sql_server_erp secret/serves)" > /convert_secret.json
    - mkdir csv
    - analytics/singer/converter_mssqlserver_csv.py --auth /convert_secret.json --output-dir csv
    - for csv_path in csv/*.csv; do
        echo "$csv_path";
        analytics/singer/streamer_csv.py "$csv_path" >> .jsonstream;
      done
    - cat .jsonstream | tap-json > .singer
    - echo "$(vault read -field=analytics_auth_redshift secret/serves)" > /target_secret.json
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "erp"
  after_script:
    - rm -rf /convert_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_MS_SQL_SERVER_ERP

sync_salesforce:
  image: registry.gitlab.com/fluidsignal/serves:ms_sql_server
  stage: analytics
  <<: *vault_login
  script:
    - pip3 install
        tap_salesforce
        analytics/singer/target_redshift
    - echo "$(vault read -field=analytics_auth_salesforce secret/serves)" > /tap_secret.json
    - echo "$(vault read -field=analytics_auth_redshift   secret/serves)" > /target_secret.json
    - tap-salesforce --config /tap_secret.json --discover > /tap_config.json
    - tap-salesforce --config /tap_secret.json --properties /tap_config.json > .singer
    - cat .singer | target-redshift --auth /target_secret.json --drop-schema --schema-name "salesforce"
  after_script:
    - rm -rf /tap_secret.json /target_secret.json
  <<: *only_schedules
  only:
    variables:
      - $FLAG_SYNC_SALESFORCE
