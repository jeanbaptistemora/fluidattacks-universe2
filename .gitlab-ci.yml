image: registry.gitlab.com/fluidsignal/web:builder

stages:
  - build
  - lint
  - deployment

builder:
  image: docker:17
  services:
    - docker:dind
  stage: build
  script:
    - docker login registry.gitlab.com -u $DOCKER_USER -p $DOCKER_PASSWD
    - docker pull registry.gitlab.com/fluidsignal/web:builder
    # Only builds from scratch if the Dockerfile has changed
    - docker build --cache-from registry.gitlab.com/fluidsignal/web:builder
      -t registry.gitlab.com/fluidsignal/web:builder builder/
    - docker push registry.gitlab.com/fluidsignal/web:builder

checks:
  stage: lint
  script:
    - if curl --fail -Lo artifacts.zip --header Private-Token:$DOCKER_PASSWD https://gitlab.com/api/v4/projects/$CI_PROJECT_ID/jobs/artifacts/$CI_COMMIT_REF_NAME/download?job=$CI_JOB_NAME; then unzip artifacts.zip && rm artifacts.zip; else echo "There are no artifacts"; fi
    - ./check-changed.sh
    - ./check-all.sh
    - ./sass-lint.sh
    - pybabel compile --directory theme/2014/translations/ --domain messages
    - for FILE in $(find . -iname '*.adoc'); do sed -i 's/^include::/include::\/builds\/fluidsignal/g' $FILE; done
    - pelican --fatal errors --fatal warnings content/
    - ./html-lint.sh
    - rm -rf output/
  artifacts:
    untracked: true
    when: on_success
    expire_in: 18 hrs
    paths:
      - cache/
  except:
    - master

pages:
  stage: deployment
  environment: staging
  script:
    - if curl --fail -Lo artifacts.zip --header Private-Token:$DOCKER_PASSWD https://gitlab.com/api/v4/projects/$CI_PROJECT_ID/jobs/artifacts/$CI_COMMIT_REF_NAME/download?job=deploy; then unzip artifacts.zip; else echo "There are no artifacts"; fi
    - sed -i 's/output\/web/public/g' xmlcombine.sh
    - sed -i 's/output/public/g' html-lint.sh draft.sh
    - for FILE in $(find . -iname '*.adoc'); do sed -i 's/^include::/include::\/builds\/fluidsignal/g' $FILE; done
    - pelican --fatal errors --fatal warnings -s pelicanconf-2018.py content/
    - ./xmlcombine.sh
    - mv public/en/blog-en public/en/blog && mv public/es/blog-es public/es/blog
    - cp -r public/es/pages-es-2018/* public/es/ && rm -rf public/es/pages-es-2018
    - cp -r public/en/pages-en-2018/* public/en/ && rm -rf public/en/pages-en-2018
    - mv public/en/redirect/index.html public/ && rmdir public/en/redirect/
    - ./draft.sh
  artifacts:
    paths:
      - public
  only:
    - master

deploy:
  stage: deployment
  environment: production
  script:
    # Download cache from previous builds
    - if curl --fail -Lo artifacts.zip --header Private-Token:$DOCKER_PASSWD https://gitlab.com/api/v4/projects/$CI_PROJECT_ID/jobs/artifacts/$CI_COMMIT_REF_NAME/download?job=$CI_JOB_NAME; then unzip artifacts.zip && rm artifacts.zip; else echo "There are no artifacts"; fi
    # Generate file for localization of the site
    - pybabel compile --directory theme/2014/translations/ --domain messages
    # Fix links to successfully include files
    - for FILE in $(find . -iname '*.adoc'); do sed -i 's/^include::/include::\/builds\/fluidsignal/g' $FILE; done
    # Generate the website, exiting on any error encountered
    - pelican --fatal errors --fatal warnings content/
    # Give the folders, named after the language of its content, a general name in each subsite
    - mv output/web/en/blog-en/* output/web/en/blog && mv output/web/es/blog-es/* output/web/es/blog
    # Script to generate a complete sitemap of the site
    - ./xmlcombine.sh
    # Remove the language identification used to group documents and images in the same directory
    - cp -r output/web/es/pages-es/* output/web/es/ && rm -rf output/web/es/pages-es
    - cp -r output/web/en/pages-en/* output/web/en/ && rm -rf output/web/en/pages-en
    # Set the redirect from web/ to web/en/
    - mv output/web/en/redirect/index.html output/web/ && rmdir output/web/en/redirect/
    # Set robots.txt
    - mv robots.txt output/web/
    # Organize images of articles with draft status
    - ./draft.sh
    # Erase content of the S3 bucket
    - aws s3 rm --recursive "s3://$S3_BUCKET_NAME"
    # Upload content to S3 bucket
    - aws s3 cp --recursive --acl=public-read-write output/ "s3://$S3_BUCKET_NAME"
    # Set 301 redirects
    - ./amz-redirect.sh
    # Erase output folder to reduce artifact size since it is untracked
    - rm -rf output/
  artifacts:
    untracked: true
    when: on_success
    expire_in: 18 hrs
    paths:
      - cache/
  only:
    - master
