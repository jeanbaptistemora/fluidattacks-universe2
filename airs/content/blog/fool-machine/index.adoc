:page-slug: fool-machine/
:page-date: 2019-08-13
:page-subtitle: Trick neural network classifiers
:page-category: machine-learning
:page-tags: machine-learning, vulnerability, code
:page-image: https://res.cloudinary.com/fluid-attacks/image/upload/v1620330876/blog/fool-machine/cover_fbydkm.webp
:page-alt: Photo by KP Bodenstein on Unsplash: https://unsplash.com/photos/ElQI4kGSbiw
:page-description: You'll see how to create images that fool classifiers into thinking they see the wrong object while maintaining visual similarity to a rightly classified image.
:page-keywords: Machine Learning, Vulnerability, Classification, Adversarial Example, Image, Artificial Intelligence, Ethical Hacking, Pentesting
:page-author: Rafael Ballestas
:page-writer: raballestasr
:name: Rafael Ballestas
:about1: Mathematician
:about2: with an itch for CS
:source: https://unsplash.com/photos/ElQI4kGSbiw


= Fool the Machine

Artificial Neural Networks (`ANNs`) are certainly a wondrous achievement.
They solve classification and other learning tasks with great accuracy.
However, they are not flawless and might misclassify certain inputs.
No problem, some error is expected.
But what if you could give it two inputs
that are virtually identical,
but you get different outputs?
Worse, what if one is correctly classified
but the other has been manipulated so that
it is classified as _anything_ you want?
Could these _adversarial examples_ be the bane of neural networks?

That is what happened with one
link:https://picoctf.com/[PicoCTF]
challenge we came across recently.
There is an application
whose sole purpose is to accept a
user-uploaded image,
classify it,
and let you know the results.
Our task was to take the image of a dog,
correctly classified as a _Malinois_,
and manipulate it so that it is classified as a tree frog.
However, for your image to be a proper _adversarial example_,
it must be perceptually indistinguishable from the original,
in other words, it must still look like the same
previously-classified dog to a human.

.link:http://2018shell.picoctf.com:11889/[Challenge] description.
image::https://res.cloudinary.com/fluid-attacks/image/upload/v1620330875/blog/fool-machine/challenge_uh7nqa.webp[Challenge description]

The applications are potentially endless.
You could:

- fool image recognition systems like physical security cameras,
as does this
link:https://github.com/advboxes/AdvBox/blob/master/applications/StealthTshirt/README.md[Stealth T-shirt].

image::https://res.cloudinary.com/fluid-attacks/image/upload/v1620330875/blog/fool-machine/stealth-shirt_dtkee4.gif[Stealth T-shirt]

- make an autonomous car crash.

image::https://res.cloudinary.com/fluid-attacks/image/upload/v1620330875/blog/fool-machine/stop-signs_s8au8t.webp[Manipulated stop signs]

- confuse virtual assistants.

image::https://res.cloudinary.com/fluid-attacks/image/upload/v1620330874/blog/fool-machine/speech-recogn_w5ytaw.webp[Trick speech recognition]

- bypass spam filters, etc.

So, how does one go about creating such an adversarial example?
Recall that in our brief survey of machine learning techniques, we discussed
[inner]#link:../crash-course-machine-learning/#artificial-neural-networks-and-deep-learning[training neural networks]#.
It is an iterative process in which
you continuously adjust the _weight_ parameters of your black box (the `ANN`)
until the outputs agree with the expected ones,
or at least, _minimize_ the cost function,
which is a measure of how wrong the prediction is.
I will borrow an image that better explains it from
an article by Adam Geitgey <<r2, [2]>>.

.Training a neural network by <<r2, [2]>>.
image::https://res.cloudinary.com/fluid-attacks/image/upload/v1620330875/blog/fool-machine/training_ezkawe.webp[Training a neural network]

This technique is known as _backpropagation_.
Now, in order to obtain a picture that is still like the original,
but will classify as something entirely different,
what one could do is add some noise;
but not too much noise, so the picture doesn't change,
and not just anywhere, but exactly in the right places,
so that the classifier reads a different pattern.
Some clever folks from Google found out that
the best way to do this is by using the gradient of the cost function.

.Adding noise to fool the classifier. From <<r1, [1]>>
image::https://res.cloudinary.com/fluid-attacks/image/upload/v1620330876/blog/fool-machine/adding-noise_gde4kd.webp[Adding noise to fool the classifier]

This is called the _fast gradient sign_ method.
This gradient can be computed
using _backpropagation_ but in reverse.
Since the model is already trained,
and we can't modify it,
let's modify the picture
little by little and
see if it gets us any closer to the target.
I will again borrow from
link:https://medium.com/@ageitgey[`@ageitgey`]
since the analogy is much clearer this way.

.Tweaking the image, by <<r2, [2]>>.
image::https://res.cloudinary.com/fluid-attacks/image/upload/v1620330876/blog/fool-machine/tweaking_nr73fb.webp[Tweaking the image]

The pseudo-code that would generate
an adversarial example via this method would be as follows.
Assume that the model is saved in a `Keras` `h5` file,
as in the challenge.
link:https://keras.io/[`Keras`] is a popular high-level
neural networks `API` for `Python`.
We can load the model,
get the input and output layers (first and last),
get the cost and gradient functions and
define a convenience function that
returns both for a particular input, like this:

.Getting cost function and gradients from a neural network
[source,python]
----
from keras.models import load_model
from keras import backend as K

model                  = load_model('model.h5')
input_layer            = model.layers[0].input
output_layer           = model.layers[-1].output
cost_function          = output_layer[0, object_type_to_fake]
gradient_function      = K.gradients(cost_function, input_layer)[0]
get_cost_and_gradients = K.function([input_layer, K.learning_phase()],
                                    [cost_function, gradient_function])
----

Where `object_type_to_fake` is the class number of
what we want to fake.
Now, according to the formula in figure 3 above,
we should add a small fraction of the
sign of the gradient, until we achieve the result.
The result should be that the confidence
in the prediction becomes at least 95%.

[source,python]
----
while confidence < 0.95:
    cost, gradient = get_cost_and_gradients([adversarial_image, 0])
    adversarial_image += 0.007 * np.sign(gradient)
----

However, this procedure takes way too long
without a `GPU`. A few hours according to Geitgey <<r2, [2]>>.
For the `CTFer` and the more practical-minded reader,
there is a library that does this and other attacks
on machine learning systems to determine their
vulnerability to adversarial examples:
link:https://github.com/tensorflow/cleverhans/[CleverHans].
Using this library,
we change the expensive `while` cycle above
to two `API` calls:
make an instance of the attack method
and then ask it to generate the adversarial example.

[source,python]
----
from cleverhans.attacks import MomentumIterativeMethod

method = MomentumIterativeMethod(model, sess=K.get_session())
test   = method.generate_np(adversarial_image, eps=0.3, eps_iter=0.06,
                            nb_iter=10, y_target=target)
----

In this case, we used a different attack, namely
the `MomentumIterativeMethod` because, in this situation,
it gives better results than the `FastGradientMethod`,
obviously also a part of `CleverHans`.
And so we obtain our adversarial example.

.Adversarial image for the challenge
image::https://res.cloudinary.com/fluid-attacks/image/upload/v1620330876/blog/fool-machine/adversarial-dog_jvm7qr.webp[Adversarial image for the challenge]

You can almost _see_ the tree frog lurking in the back,
if you imagine the two knobs on the cabinet are its eyes.
Just kidding.
Upload it to the challenge site and,
instead of getting the predictions, we get the flag.

Not just that, the model which is based on
link:https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html[`MobileNet`],
is 99.99974% certain that this is a tree frog.
However, the difference between it and the original image,
according to the widely used
perceptual hash algorithm,
is less than two bits.
Still, the adversarial example has artifacts,
at least to a human observer.

What is worse is that these issues persist
_across_ different models
as long as the training data is similar.
That means that we could probably pass the same image
to a different animal image classifier
and still get the same results.

Ultimately, we should think twice before deploying
`ML`-powered security measures.
This is, of course, a mock example,
but in more critical situations,
having models that are not resistant to
adversarial examples could result in catastrophic effects.
Apparently<<r1, ^[1]^>>,
the reason behind this is the
linearity within the functions hidden in these networks.
So switching to a more non-linear model, such as
link:https://en.wikipedia.org/wiki/Radial_basis_function_network[RBF networks],
could solve the problem.
Another workaround could be to train the
`ANNs` _including_ adversarial examples.

To borrow a phrase from carpenters,
"Measure twice, cut once." We should also remember
that whatever the solution,
it should be clear that one should test twice, and deploy once.

== References

. [[r1]] I. Goodfellow, J. Shlens, C. Szegedy.
_EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES_.
link:https://arxiv.org/pdf/1412.6572.pdf[arXiv].

. [[r2]] A. Geigtey.
_Machine Learning is Fun Part 8: How to Intentionally Trick Neural Networks_.
link:https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196[Medium]
