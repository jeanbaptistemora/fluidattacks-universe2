import html
import json
import os
import uuid
from itertools import chain
from operator import itemgetter
from typing import (
    Awaitable,
    Dict,
    List,
    Optional,
    Set,
    cast,
)
from urllib.parse import urlparse

import yaml
from aioextensions import collect
from graphql.type.definition import GraphQLResolveInfo
from pykwalify.core import Core
from pykwalify.errors import (
    CoreError,
    SchemaError,
)
from starlette.datastructures import UploadFile

from custom_exceptions import (
    InvalidFileSize,
    InvalidPath,
    InvalidPort,
    InvalidSchema,
    InvalidSpecific,
    InvalidStream,
    InvalidVulnsNumber,
)
from custom_types import (
    Finding as FindingType,
    User as UserType,
    Vulnerability as VulnerabilityType,
)
from dynamodb.types import OrgFindingPolicyItem
from findings import domain as findings_domain
from newutils import (
    datetime as datetime_utils,
    files as files_utils,
    logs as logs_utils,
    requests as requests_utils,
    token as token_utils,
    vulnerabilities as vulns_utils,
)
from roots import domain as roots_domain
from vulnerabilities import (
    dal as vulns_dal,
    domain as vulns_domain,
)


async def _validate_roots(
    *, group_name: str, vulnerabilities: Dict[str, List[Dict[str, str]]]
) -> None:
    nicknames = set(
        vuln["repo_nickname"]
        for vulns in vulnerabilities.values()
        for vuln in vulns
        if "repo_nickname" in vuln
    )
    await collect(
        tuple(
            roots_domain.get_root_by_nickname(
                group_name=group_name, repo_nickname=nickname
            )
            for nickname in nicknames
        )
    )


def get_treatment_new_vuln(
    *, current_day: str, finding_policy: Optional[OrgFindingPolicyItem]
) -> List[Dict[str, str]]:
    if finding_policy and finding_policy.state.status == "APPROVED":
        return vulns_utils.get_treatment_from_org_finding_policy(
            current_day=current_day,
            user_email=finding_policy.state.modified_by,
        )

    return [{"date": current_day, "treatment": "NEW"}]


async def add_vuln_to_dynamo(
    *,
    item: Dict[str, str],
    specific: str,
    finding_id: str,
    info: GraphQLResolveInfo,
    vulnerability: Optional[Dict[str, FindingType]],
    finding_policy: Optional[OrgFindingPolicyItem],
) -> bool:
    """Add vulnerability to dynamo."""
    historic_state = []
    response = False
    current_day = datetime_utils.get_now_as_str()
    user_data = cast(UserType, await token_utils.get_jwt_content(info.context))
    email = str(user_data["user_email"])
    if vulnerability:
        return await vulns_domain.update_vuln_state(
            info=info,
            vulnerability=vulnerability,
            item=item,
            finding_id=finding_id,
            current_day=current_day,
            finding_policy=finding_policy,
        )

    data: Dict[str, FindingType] = {"repo_nickname": item["repo_nickname"]}
    source = requests_utils.get_source(info.context)
    new_state: Dict[str, str] = {
        "analyst": email,
        "date": current_day,
        "source": source,
    }
    data["historic_treatment"] = get_treatment_new_vuln(
        current_day=current_day,
        finding_policy=finding_policy,
    )
    data["vuln_type"] = item.get("vuln_type", "")
    data["where"] = item.get("where", "")
    data["specific"] = specific
    data["finding_id"] = finding_id
    if "stream" in item:
        data["stream"] = item["stream"]
    if "commit_hash" in item:
        data["commit_hash"] = item["commit_hash"]
    data["UUID"] = str(uuid.uuid4())
    if item.get("state"):
        new_state["state"] = item["state"]
        historic_state.append(new_state)
        data["historic_state"] = historic_state
        response = await vulns_dal.create(data)
    else:
        logs_utils.cloudwatch_log(
            info.context,
            "Security: Attempted to add vulnerability without state",
        )
    return response


async def get_vulns_to_add(
    vulnerabilities: Dict[str, List[VulnerabilityType]],
) -> List[Dict[str, str]]:
    coroutines = []
    for vuln_type in ["inputs", "lines", "ports"]:
        file_vuln = vulnerabilities.get(vuln_type)
        if file_vuln:
            coroutines.extend(
                [map_vulnerability_type(vuln, vuln_type) for vuln in file_vuln]
            )
        else:
            pass
            # If a file does not have a type of vulnerabilities,
            # this does not represent an error or an exceptional condition.
    return list(chain.from_iterable(await collect(coroutines)))


async def map_vulnerability_type(
    item: VulnerabilityType,
    vuln_type: str,
) -> List[Dict[str, str]]:
    """Add vulnerability to dynamo."""
    response = []
    where_headers = {
        "inputs": {"where": "url", "specific": "field"},
        "lines": {"where": "path", "specific": "line"},
        "ports": {"where": "host", "specific": "port"},
    }

    data: Dict[str, str] = {
        "repo_nickname": item.get("repo_nickname"),
        "state": str(item.get("state", "")),
        "vuln_type": vuln_type,
    }
    data["where"] = str(item.get(where_headers[vuln_type]["where"]))
    specific = str(item.get(where_headers[vuln_type]["specific"]))

    if vuln_type == "lines" and data["where"].find("\\") >= 0:
        path = data["where"].replace("\\", "\\\\")
        raise InvalidPath(expr=f'"values": "{path}"')
    if "stream" in item:
        validate_stream(data["where"], str(item["stream"]))
        data["stream"] = str(item["stream"])
    if "commit_hash" in item:
        data["commit_hash"] = str(item["commit_hash"])
    if vulns_utils.is_range(specific) or vulns_utils.is_sequence(specific):
        response.extend(
            ungroup_vulnerability_specific(vuln_type, specific, data)
        )
    else:
        if vuln_type == "ports" and not 0 <= int(specific) <= 65535:
            raise InvalidPort(expr=f'"values": "{specific}"')
        response.append({**data, "specific": specific})
    return response


def _get_vulns_to_reattack(
    last_state: str,
    new_state: str,
    vulns_in_db: List[Dict[str, FindingType]],
    vulns_to_add: List[Dict[str, str]],
) -> List[Dict[str, str]]:
    return sorted(
        [
            {**vuln_to_add, "UUID": vuln_in_db["UUID"]}
            for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add)
            if (
                vulns_utils.is_reattack_requested(vuln_in_db)
                and vulns_utils.get_last_status(vuln_in_db) == last_state
                and vuln_to_add.get("state") == new_state
            )
        ],
        key=itemgetter("UUID"),
    )


def _get_vulns_uuids(vulns: List[Dict[str, str]]) -> Set[str]:
    return set(map(itemgetter("UUID"), vulns))


async def map_vulns_to_dynamo(
    info: GraphQLResolveInfo,
    vulnerabilities: Dict[str, List[VulnerabilityType]],
    finding: Dict[str, FindingType],
    finding_policy: Optional[OrgFindingPolicyItem],
) -> bool:
    """Map vulnerabilities and send it to dynamo."""
    finding_id: str = finding["finding_id"]

    # Vulns uploaded by the user
    vulns_to_add = await get_vulns_to_add(vulnerabilities)
    if len(vulns_to_add) > 100:
        raise InvalidVulnsNumber()

    # Vulns as they appear in the DB
    finding_vulns = await vulns_dal.get_by_finding(str(finding_id))
    vulns_in_db: List[Dict[str, FindingType]] = [
        next(
            iter(
                [
                    vuln
                    for vuln in finding_vulns
                    if (
                        vuln_to_add.get("vuln_type", "")
                        == vuln.get("vuln_type", "_")
                        and vuln_to_add.get("where", "")
                        == vuln.get("where", "_")
                        and vuln_to_add.get("specific", "")
                        == vuln.get("specific", "_")
                    )
                ]
            ),
            {},
        )
        for vuln_to_add in vulns_to_add
    ]

    # Vulnerabilities that have a requested reattack are managed differently
    # They need to have their historic_verification modified so we avoid
    # open vulns being closed in this batch from being pending to reattack
    closed_vulns_to_reattack: List[Dict[str, str]] = _get_vulns_to_reattack(
        last_state="open",
        new_state="closed",
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    closed_vulns_uuids = _get_vulns_uuids(closed_vulns_to_reattack)

    coroutines: List[Awaitable[bool]] = [
        add_vuln_to_dynamo(
            item=vuln_to_add,
            specific=vuln_to_add["specific"],
            finding_id=finding_id,
            info=info,
            vulnerability=vuln_in_db,
            finding_policy=finding_policy,
        )
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        # Exclude vulns that we'll verify as those state updates
        # are handled in the verification function
        if vuln_in_db.get("UUID") not in closed_vulns_uuids
    ]

    user_data = cast(UserType, await token_utils.get_jwt_content(info.context))

    if closed_vulns_to_reattack:
        coroutines.append(
            findings_domain.verify_vulnerabilities(
                info=info,
                finding_id=finding_id,
                user_info=user_data,
                parameters={
                    "justification": "The vulnerability was verified"
                    " by closing it",
                    "closed_vulns": list(closed_vulns_uuids),
                },
                vulns_to_close_from_file=closed_vulns_to_reattack,
            )
        )

    return all(await collect(coroutines))


async def process_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding: Dict[str, FindingType],
    finding_policy: Optional[OrgFindingPolicyItem],
) -> bool:
    """Process a file."""
    success = False
    finding_id = finding.get("finding_id", "")
    group_name: str = finding["project_name"]
    raw_content = await file_input.read()
    raw_content = cast(bytes, raw_content).decode()
    file_content = html.escape(raw_content, quote=False)
    await file_input.seek(0)
    vulnerabilities = yaml.safe_load(file_content)
    file_url = f"/tmp/vulnerabilities-{uuid.uuid4()}-{finding_id}.yaml"
    with open(file_url, "w") as stream:
        yaml.safe_dump(vulnerabilities, stream)
    if validate_file_schema(file_url, info):
        await _validate_roots(
            group_name=group_name, vulnerabilities=vulnerabilities
        )
        success = await map_vulns_to_dynamo(
            info, vulnerabilities, finding, finding_policy
        )
    else:
        success = False
    return success


def ungroup_vulnerability_specific(
    vuln: str, specific: str, data: Dict[str, str]
) -> List[Dict[str, str]]:
    """Add vulnerability auxiliar."""
    if vuln in ("lines", "ports"):
        specific_values = vulns_utils.ungroup_specific(specific)
    else:
        specific_values = [spec for spec in specific.split(",") if spec]
    if vuln == "ports" and not all(
        (0 <= int(i) <= 65535) for i in specific_values
    ):
        error_value = f'"values": "{specific}"'
        raise InvalidPort(expr=error_value)
    if not specific_values:
        raise InvalidSpecific()
    return [{**data, "specific": specific} for specific in specific_values]


async def upload_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding: Dict[str, FindingType],
    finding_policy: Optional[OrgFindingPolicyItem],
) -> bool:
    mib = 1048576
    success = False
    if await files_utils.get_file_size(file_input) < 1 * mib:
        success = await process_file(info, file_input, finding, finding_policy)
    else:
        raise InvalidFileSize()
    return success


def validate_file_schema(file_url: str, info: GraphQLResolveInfo) -> bool:
    """Validate if a file has the correct schema."""
    schema_dir = os.path.dirname(os.path.abspath(__file__))
    schema_dir = os.path.join(schema_dir, "vuln_template.yaml")
    core = Core(source_file=file_url, schema_files=[schema_dir])
    is_valid = False
    try:
        core.validate(raise_exception=True)
        is_valid = True
    except SchemaError:
        lines_of_exceptions = core.errors
        errors_values = [
            getattr(x, "pattern", "")
            for x in lines_of_exceptions
            if not hasattr(x, "key")
        ]
        errors_keys = [x for x in lines_of_exceptions if hasattr(x, "key")]
        errors_values_formated = [json.dumps(x) for x in errors_values]
        errors_keys_formated = [f'"{x.key}"' for x in errors_keys]
        errors_keys_joined = ",".join(errors_keys_formated)
        errors_values_joined = ",".join(errors_values_formated)
        error_value = (
            f'"values": [{errors_values_joined}], '
            f'"keys": [{errors_keys_joined}]'
        )
        logs_utils.cloudwatch_log(
            info.context,
            "Error: An error occurred validating vulnerabilities file",
        )
        raise InvalidSchema(expr=error_value)
    except CoreError:
        raise InvalidSchema()
    finally:
        os.unlink(file_url)
    return is_valid


def validate_stream(where: str, stream: str) -> bool:
    url_parsed = urlparse(where)
    if len(url_parsed.path) == 0 or url_parsed.path == "/":
        if stream.lower().startswith("home,"):
            return True
        raise InvalidStream()
    return True
