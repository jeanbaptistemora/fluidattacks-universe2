# SPDX-FileCopyrightText: 2022 Fluid Attacks <development@fluidattacks.com>
#
# SPDX-License-Identifier: MPL-2.0

from .dal import (
    sign_vuln_file_url,
    upload_vuln_file_to_s3,
)
from aioextensions import (
    collect,
    schedule,
)
from custom_exceptions import (
    AlreadyZeroRiskConfirmed,
    ExpectedPathToStartWithRepo,
    InvalidCannotModifyNicknameWhenClosing,
    InvalidFileSize,
    InvalidNewVulnState,
    InvalidPath,
    InvalidPort,
    InvalidSchema,
    InvalidSpecific,
    InvalidVulnsNumber,
    LineDoesNotExistInTheLinesOfCodeRange,
    MachineCanNotOperate,
    RootNotFound,
    ToeInputNotFound,
    ToeLinesNotFound,
    VulnerabilityPathDoesNotExistInToeLines,
    VulnerabilityUrlFieldDoNotExistInToeInputs,
)
from db_model import (
    vulnerabilities as vulns_model,
)
from db_model.enums import (
    Source,
)
from db_model.findings.types import (
    Finding,
)
from db_model.roots.enums import (
    RootStatus,
)
from db_model.roots.types import (
    Root,
)
from db_model.toe_inputs.types import (
    ToeInputRequest,
)
from db_model.toe_lines.types import (
    ToeLines,
    ToeLinesRequest,
)
from db_model.vulnerabilities.enums import (
    VulnerabilityStateStatus,
    VulnerabilityToolImpact,
    VulnerabilityTreatmentStatus,
    VulnerabilityType,
    VulnerabilityZeroRiskStatus,
)
from db_model.vulnerabilities.types import (
    Vulnerability,
    VulnerabilityMetadataToUpdate,
    VulnerabilityState,
    VulnerabilityTool,
    VulnerabilityTreatment,
    VulnerabilityUnreliableIndicators,
)
from dynamodb.types import (
    OrgFindingPolicyItem,
)
from findings import (
    domain as findings_domain,
)
from graphql.type.definition import (
    GraphQLResolveInfo,
)
import html
from itertools import (
    chain,
)
import json
from machine.availability import (
    operation_can_be_executed,
)
from newutils import (
    datetime as datetime_utils,
    files as files_utils,
    logs as logs_utils,
    token as token_utils,
    validations,
    vulnerabilities as vulns_utils,
)
from operator import (
    attrgetter,
)
import os
from pykwalify.core import (
    Core,
)
from pykwalify.errors import (
    CoreError,
    SchemaError,
)
import re
from roots import (
    domain as roots_domain,
)
from starlette.datastructures import (
    UploadFile,
)
from typing import (
    Any,
    Dict,
    List,
    Optional,
    Set,
    Tuple,
    Union,
)
import uuid
from vulnerabilities import (
    domain as vulns_domain,
)
from vulnerabilities.domain.utils import (
    get_hash_from_typed,
)
from vulnerabilities.domain.validations import (
    validate_stream,
)
from vulnerabilities.types import (
    ToolItem,
)
import yaml


def _get_treatment_new_vuln(
    *, modified_date: str, finding_policy: Optional[OrgFindingPolicyItem]
) -> Tuple[VulnerabilityTreatment, ...]:
    if finding_policy and finding_policy.state.status == "APPROVED":
        return vulns_utils.get_treatment_from_org_finding_policy(
            modified_date=modified_date,
            user_email=finding_policy.state.modified_by,
        )
    return (
        VulnerabilityTreatment(
            modified_date=modified_date,
            status=VulnerabilityTreatmentStatus.NEW,
        ),
    )


async def _add_vulnerability_to_dynamo(
    *,
    vuln_to_add: Vulnerability,
    finding_policy: Optional[OrgFindingPolicyItem],
) -> str:
    """Add new vulnerability to DynamoDB."""
    if vuln_to_add.state.status != VulnerabilityStateStatus.OPEN:
        raise InvalidNewVulnState.new()

    new_historic_treatment = _get_treatment_new_vuln(
        modified_date=vuln_to_add.state.modified_date,
        finding_policy=finding_policy,
    )
    new_id = str(uuid.uuid4())
    unreliable_indicators = VulnerabilityUnreliableIndicators(
        unreliable_source=vuln_to_add.state.source,
        unreliable_treatment_changes=vulns_utils.get_treatment_changes(
            new_historic_treatment
        ),
    )
    new_vulnerability = vuln_to_add._replace(
        id=new_id,
        treatment=new_historic_treatment[-1],
        unreliable_indicators=unreliable_indicators,
    )
    await vulns_model.add(vulnerability=new_vulnerability)
    if len(new_historic_treatment) > 1:
        await vulns_model.update_historic(
            current_value=new_vulnerability,
            historic=new_historic_treatment,
        )
    return new_id


def _deduplicate_vulns(
    vulns: List[Vulnerability],
) -> List[Vulnerability]:
    # In case there are repeated vulns, only the latest will be taken into
    # account
    return list(
        {
            get_hash_from_typed(vuln, from_yaml=True): vuln for vuln in vulns
        }.values()
    )


async def _get_vulns_to_add(  # pylint: disable=too-many-arguments
    loaders: Any,
    vulns_data_from_file: dict[str, list[dict[str, Any]]],
    group_name: str,
    finding_id: str,
    today: str,
    user_email: str,
) -> List[Vulnerability]:
    """Get typed vulnerabilities from the data processed from the yaml file."""
    roots: Tuple[Root, ...] = await loaders.group_roots.load(group_name)
    root_ids_by_nicknames = roots_domain.get_root_ids_by_nicknames(
        roots, only_git_roots=False
    )
    return _deduplicate_vulns(
        list(
            chain.from_iterable(
                map_vulnerability_type(
                    item=vuln,
                    finding_id=finding_id,
                    group_name=group_name,
                    root_ids_by_nicknames=root_ids_by_nicknames,
                    today=today,
                    user_email=user_email,
                    vuln_type=vuln_type,
                )
                for vuln_type in ["inputs", "lines", "ports"]
                for vuln in vulns_data_from_file.get(vuln_type, [])
            )
        )
    )


def map_vulnerability_type(  # noqa: MC0001
    item: dict[str, Any],
    finding_id: str,
    group_name: str,
    root_ids_by_nicknames: dict[str, str],
    today: str,
    user_email: str,
    vuln_type: str,
) -> list[Vulnerability]:
    """Map fields according to vulnerability type and expand data if specific
    is a range or sequence.
    """
    where_headers = {
        "inputs": {"where": "url", "specific": "field"},
        "lines": {"where": "path", "specific": "line"},
        "ports": {"where": "host", "specific": "port"},
    }
    where: str = item[where_headers[vuln_type]["where"]]
    specific: str = item[where_headers[vuln_type]["specific"]]

    commit_hash: Optional[str] = None
    if vuln_type == "lines":
        commit_hash = str(item["commit_hash"])

        if not where.startswith(item["repo_nickname"]):
            raise ExpectedPathToStartWithRepo.new()

        # Use Unix-like paths
        if where.find("\\") >= 0:
            path = where.replace("\\", "\\\\")
            raise InvalidPath(expr=f'"values": "{path}"')

    stream: Optional[str] = None
    if vuln_type == "inputs":
        stream = str(item["stream"])
        validate_stream(where, stream)

        if item.get("commit_hash"):
            commit_hash = str(item["commit_hash"])

    if vuln_type == "ports" and not 0 <= int(specific) <= 65535:
        raise InvalidPort(expr=f'"values": "{specific}"')

    validations.validate_sanitized_csv_input(specific)
    vuln_stream = stream.split(",") if stream else None
    tool = (
        VulnerabilityTool(
            name=item["tool"]["name"],
            impact=VulnerabilityToolImpact[
                str(item["tool"]["impact"]).upper()
            ],
        )
        if "tool" in item
        else None
    )
    # Vulnerability id(UUID4) will be replaced later, once the comparison with
    # existing vulns is done
    vulnerability = Vulnerability(
        created_by=user_email,
        created_date=today,
        finding_id=finding_id,
        group_name=group_name,
        hacker_email=user_email,
        id="",
        specific=specific,
        state=VulnerabilityState(
            modified_by=user_email,
            modified_date=today,
            source=Source[str(item["source"]).upper()],
            status=VulnerabilityStateStatus[str(item["state"]).upper()],
            tool=tool,
        ),
        type=VulnerabilityType[vuln_type.upper()],
        where=where,
        root_id=roots_domain.get_root_id_by_nicknames(
            nickname=item["repo_nickname"],
            root_ids_by_nicknames=root_ids_by_nicknames,
        ),
        skims_method=item.get("skims_method"),
        skims_technique=item.get("skims_technique"),
        developer=item.get("developer"),
        commit=commit_hash,
        stream=vuln_stream,
    )

    if vulns_utils.is_range(specific) or vulns_utils.is_sequence(specific):
        return ungroup_vulnerability_specific(
            vuln_type, specific, vulnerability
        )

    return [vulnerability]


def _get_vulns_to_reattack(
    last_status: VulnerabilityStateStatus,
    new_status: VulnerabilityStateStatus,
    vulns_in_db: List[Optional[Vulnerability]],
    vulns_to_add: List[Vulnerability],
) -> List[Vulnerability]:
    return sorted(
        [
            vuln_to_add._replace(id=vuln_in_db.id)
            for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add)
            if (
                vuln_in_db
                and vulns_utils.is_reattack_requested(vuln_in_db)
                and vuln_in_db.state.status == last_status
                and vuln_to_add.state.status == new_status
            )
        ],
        key=attrgetter("id"),
    )


def _get_vulns_ids(vulns: List[Vulnerability]) -> Set[str]:
    return set(map(attrgetter("id"), vulns))


async def _validate_nicknames(
    *,
    loaders: Any,
    group: str,
    vulns_in_db: List[Optional[Vulnerability]],
    vulns_to_add: List[Vulnerability],
) -> None:
    root_ids: set[str] = {
        root.id
        for root in await loaders.group_roots.load(group)
        if root.state.status == RootStatus.ACTIVE
    }
    for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add):
        # You can only report open vulns in an existing root
        if (
            vuln_to_add.state.status == VulnerabilityStateStatus.OPEN
            and vuln_to_add.root_id not in root_ids
        ):
            raise RootNotFound()
        # When closing you can set the nickname if it does not exist
        # otherwise it must be equal to the existent nickname
        if vuln_to_add.state.status == VulnerabilityStateStatus.CLOSED:
            if vuln_in_db and vuln_in_db.root_id:
                if vuln_in_db.root_id != vuln_to_add.root_id:
                    raise InvalidCannotModifyNicknameWhenClosing.new()
            else:
                if vuln_to_add.root_id not in root_ids:
                    raise RootNotFound()


def _sort_vulns_for_comparison(
    vulns: List[Vulnerability],
) -> List[Vulnerability]:
    """Sort vulns, opened first, nickname last"""
    # Open vulns have priority so they can be closed
    # No-Nickname vulns have priority so they can be set their nickname
    return sorted(
        vulns,
        key=lambda vuln: (
            # Open ones first
            vuln.state.status != VulnerabilityStateStatus.OPEN,
            # Nickname last
            bool(vuln.root_id),
        ),
    )


async def map_vulnerabilities_to_dynamo(  # pylint: disable=too-many-locals
    *,
    loaders: Any,
    vulns_data_from_file: dict[str, list[dict[str, Any]]],
    group_name: str,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    context: Optional[Any] = None,
    **kwargs: Any,
) -> Set[str]:
    """Map vulnerabilities, send them to DynamoDB, verify them and return the
    processed vulnerabilities."""
    user_info = (
        await token_utils.get_jwt_content(context)
        if context
        else kwargs["user_info"]
    )
    user_email = user_info["user_email"]
    today = datetime_utils.get_iso_date()
    # Vulns uploaded by the user
    vulns_to_add: List[Vulnerability] = await _get_vulns_to_add(
        loaders=loaders,
        vulns_data_from_file=vulns_data_from_file,
        group_name=group_name,
        finding_id=finding_id,
        today=today,
        user_email=user_email,
    )

    # Avoid DoS
    if len(vulns_to_add) > 100 and kwargs.get("raise_validation", True):
        raise InvalidVulnsNumber()

    # Vulns as they appear in the DB
    finding_vulns_loader = loaders.finding_vulnerabilities
    finding_vulns = await finding_vulns_loader.load(finding_id)
    sorted_vulns = _sort_vulns_for_comparison(finding_vulns)
    vulns_in_db: List[Optional[Vulnerability]] = [
        next(
            (
                vuln
                for vuln in sorted_vulns
                if get_hash_from_typed(vuln_to_add)
                == get_hash_from_typed(vuln, from_yaml=True)
                and (
                    vuln.state.status == VulnerabilityStateStatus.OPEN
                    or (
                        vuln.state.status == VulnerabilityStateStatus.CLOSED
                        and vuln_to_add.state.status
                        == VulnerabilityStateStatus.CLOSED
                    )
                )
            ),
            None,
        )
        for vuln_to_add in vulns_to_add
    ]

    # Validate vulnerability roots
    await _validate_nicknames(
        loaders=loaders,
        group=group_name,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    if kwargs.get("raise_validation", True):
        # Validate no uploaded vulns with ZR status
        for vuln_in_db in vulns_in_db:
            if (
                vuln_in_db
                and vuln_in_db.zero_risk is not None
                and vuln_in_db.zero_risk.status
                == VulnerabilityZeroRiskStatus.CONFIRMED
            ):
                raise AlreadyZeroRiskConfirmed(
                    info=(
                        f"Where: {vuln_in_db.where}\n"
                        f"Specific: {vuln_in_db.specific}"
                    )
                )
    else:
        vulns_in_db = [
            vuln_in_db
            for vuln_in_db in vulns_in_db
            if not (
                vuln_in_db
                and vuln_in_db.zero_risk is not None
                and vuln_in_db.zero_risk.status
                == VulnerabilityZeroRiskStatus.CONFIRMED
            )
        ]

    # Vulnerabilities that have a requested reattack are managed differently
    # They need to have their historic_verification modified so we avoid
    # open vulns being closed in this batch from being pending to reattack
    closed_vulns_to_reattack: List[Vulnerability] = _get_vulns_to_reattack(
        last_status=VulnerabilityStateStatus.OPEN,
        new_status=VulnerabilityStateStatus.CLOSED,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    open_vulns_to_reattack: List[Vulnerability] = _get_vulns_to_reattack(
        last_status=VulnerabilityStateStatus.OPEN,
        new_status=VulnerabilityStateStatus.OPEN,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    closed_vulns_ids: Set[str] = _get_vulns_ids(closed_vulns_to_reattack)
    open_vulns_ids: Set[str] = _get_vulns_ids(open_vulns_to_reattack)

    vulns_to_add_to_dynamo: Tuple[Optional[Vulnerability], ...] = tuple(
        vuln_to_add
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        if not vuln_in_db
        and vuln_to_add.state.status != VulnerabilityStateStatus.CLOSED
    )
    # Validate new vulnerabilities exist in the toe (surface)
    vulns_to_add_to_dynamo_list: List[Optional[Vulnerability]] = [
        await validate_vulnerability_in_toe(
            loaders,
            group_name,
            vuln_to_add,
            raises=kwargs.get("raise_validation", True),
        )
        for vuln_to_add in vulns_to_add_to_dynamo
        if vuln_to_add
    ]
    # Add new vulnerabilities
    added_vuln_ids = set(
        await collect(
            _add_vulnerability_to_dynamo(
                vuln_to_add=vuln_to_add,
                finding_policy=finding_policy,
            )
            for vuln_to_add in tuple(vulns_to_add_to_dynamo_list)
            if vuln_to_add
        )
    )

    # Update those vulns that will not be reattacked
    updated_vuln_ids = set(
        await collect(
            vulns_domain.update_metadata_and_state(
                vulnerability=vuln_in_db,
                new_metadata=VulnerabilityMetadataToUpdate(
                    commit=vuln_to_add.commit,
                    root_id=vuln_to_add.root_id,
                    stream=vuln_to_add.stream,
                ),
                new_state=vuln_to_add.state,
                finding_policy=finding_policy,
            )
            for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
            # Exclude vulns that we'll verify as those state updates
            # are handled in the verification function
            if vuln_in_db
            and vuln_in_db.id not in closed_vulns_ids
            and vuln_in_db.id not in open_vulns_ids
        )
    )

    if closed_vulns_to_reattack:
        await findings_domain.verify_vulnerabilities(
            context=context,
            finding_id=finding_id,
            user_info=user_info,
            justification=(
                "The vulnerability was re-attacked and found to be closed"
            ),
            open_vulns_ids=[],
            closed_vulns_ids=list(closed_vulns_ids),
            vulns_to_close_from_file=closed_vulns_to_reattack,
            is_reattack_open=False,
            loaders=loaders,
        )

    if open_vulns_to_reattack:
        await findings_domain.verify_vulnerabilities(
            context=context,
            finding_id=finding_id,
            user_info=user_info,
            justification=(
                "The vulnerability was re-attacked and found to be still open"
            ),
            open_vulns_ids=list(open_vulns_ids),
            closed_vulns_ids=[],
            vulns_to_close_from_file=[],
            is_reattack_open=True,
            loaders=loaders,
        )

    # Get locations
    vulns_props: dict[str, Any] = {
        vuln_id: {"Location": vuln.where, "Specific": vuln.specific}
        for vuln_id, vuln in zip(added_vuln_ids, vulns_to_add_to_dynamo)
        if vuln
    }

    # Send mail report
    if vulns_props:
        schedule(
            findings_domain.send_vulnerability_report(
                loaders=loaders,
                finding_id=finding_id,
                vulnerabilities_properties=vulns_props,
            )
        )

    return set.union(
        updated_vuln_ids, closed_vulns_ids, open_vulns_ids, added_vuln_ids
    )


async def process_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> Set[str]:
    """Process a file and return the for vulnerabilities that were
    processed."""
    raw_content = await file_input.read()
    raw_content = raw_content.decode()  # type: ignore
    file_content = html.escape(raw_content, quote=False)
    await file_input.seek(0)
    vulnerabilities = yaml.safe_load(file_content)
    # FP: the generated filename is unpredictable
    file_url = (  # NOSONAR # nosec # noqa: E501
        f"/tmp/vulnerabilities-{uuid.uuid4()}-{finding_id}.yaml"
    )
    with open(file_url, "w", encoding="utf-8") as stream:
        yaml.safe_dump(vulnerabilities, stream)
    if validate_file_schema(file_url, info):
        return await map_vulnerabilities_to_dynamo(
            context=info.context,
            vulns_data_from_file=vulnerabilities,
            group_name=group_name,
            finding_id=finding_id,
            finding_policy=finding_policy,
            loaders=info.context.loaders,
            raise_validation=True,
        )
    return set()


def ungroup_vulnerability_specific(
    vuln_type: str, specific: str, vulnerability: Vulnerability
) -> List[Vulnerability]:
    """Add vulnerability auxiliar."""
    if vuln_type in ("lines", "ports"):
        specific_values = vulns_utils.ungroup_specific(specific)
    else:
        specific_values = [spec for spec in specific.split(",") if spec]
    if vuln_type == "ports" and not all(
        (0 <= int(i) <= 65535) for i in specific_values
    ):
        error_value = f'"values": "{specific}"'
        raise InvalidPort(expr=error_value)
    if not specific_values:
        raise InvalidSpecific()
    return [
        vulnerability._replace(specific=specific)
        for specific in specific_values
    ]


async def upload_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> Set[str]:
    """Upload the vulnerabilities and return the vulnerabilities that were
    processed."""
    finding_loader = info.context.loaders.finding
    finding: Finding = await finding_loader.load(finding_id)
    if not operation_can_be_executed(info.context, finding.title):
        raise MachineCanNotOperate()

    mib = 1048576
    if await files_utils.get_file_size(file_input) < 1 * mib:
        processed_vulnerabilities = await process_file(
            info,
            file_input,
            finding_id,
            finding_policy,
            group_name,
        )
    else:
        raise InvalidFileSize()
    return processed_vulnerabilities


def validate_file_schema(file_url: str, info: GraphQLResolveInfo) -> bool:
    """Validate if a file has the correct schema."""
    schema_dir = os.path.dirname(os.path.abspath(__file__))
    schema_dir = os.path.join(schema_dir, "vuln_template.yaml")
    core = Core(source_file=file_url, schema_files=[schema_dir])
    is_valid = False
    try:
        core.validate(raise_exception=True)
        is_valid = True
    except SchemaError as ex:
        lines_of_exceptions = core.errors
        errors_values = [
            getattr(x, "pattern", getattr(x, "value", ""))
            for x in lines_of_exceptions
            if not hasattr(x, "key")
        ]
        errors_keys = [x for x in lines_of_exceptions if hasattr(x, "key")]
        errors_values_formated = [json.dumps(x) for x in errors_values]
        errors_keys_formated = [f'"{x.key}"' for x in errors_keys]
        errors_keys_joined = ",".join(errors_keys_formated)
        errors_values_joined = ",".join(errors_values_formated)
        error_value = (
            f'"values": [{errors_values_joined}], '
            f'"keys": [{errors_keys_joined}]'
        )
        logs_utils.cloudwatch_log(
            info.context,
            "Error: An error occurred validating vulnerabilities file",
        )
        raise InvalidSchema(expr=error_value) from ex
    except CoreError as ex:
        raise InvalidSchema() from ex
    finally:
        os.unlink(file_url)
    return is_valid


async def validate_vulnerability_in_toe(  # noqa: MC0001
    loaders: Any,
    group_name: str,
    vulnerability: Vulnerability,
    raises: bool = True,
) -> Optional[Vulnerability]:
    if vulnerability.root_id:
        where = html.unescape(vulnerability.where)
        # There are cases, like SCA vulns, where the `where` attribute
        # has additional information `filename (pacakge) [CVE]`
        if match := re.match(r"(?P<where>.*)\s\(.*\)(\s\[.*\])?$", where):
            where = match.groupdict()["where"]

        if vulnerability.type == VulnerabilityType.LINES:
            slash_index = where.find("/")
            if slash_index == -1:
                if raises:
                    raise VulnerabilityPathDoesNotExistInToeLines()
                return None
            filename = where[slash_index + 1 :]
            try:
                toe_lines: ToeLines = await loaders.toe_lines.load(
                    ToeLinesRequest(
                        filename=filename,
                        group_name=group_name,
                        root_id=vulnerability.root_id,
                    )
                )
            except ToeLinesNotFound as exc:
                if raises:
                    raise VulnerabilityPathDoesNotExistInToeLines() from exc
                return None

            if not 0 <= int(vulnerability.specific) <= toe_lines.loc:
                if raises:
                    raise LineDoesNotExistInTheLinesOfCodeRange(
                        vulnerability.specific
                    )
                return None

        if vulnerability.type == VulnerabilityType.INPUTS:
            specific = html.unescape(vulnerability.specific)
            if match_specific := re.match(
                r"(?P<specific>.*)\s\(.*\)(\s\[.*\])?$", specific
            ):
                specific = match_specific.groupdict()["specific"]

            try:
                await loaders.toe_input.load(
                    ToeInputRequest(
                        component=where,
                        entry_point=""
                        if vulnerability.state.source == Source.MACHINE
                        else specific,
                        group_name=group_name,
                        root_id=vulnerability.root_id,
                    )
                )
            except ToeInputNotFound as exc:
                if vulnerability.skims_technique not in {"APK"}:
                    if raises:
                        raise VulnerabilityUrlFieldDoNotExistInToeInputs() from exc  # noqa
                    return None
        return vulnerability
    return None


async def get_vulnerabilities_by_type(
    loaders: Any, finding_id: str
) -> Dict[str, List[Dict[str, Union[str, ToolItem]]]]:
    """Get vulnerabilities group by type."""
    vulnerabilities = await loaders.finding_vulnerabilities_nzr.load(
        finding_id
    )
    finding: Finding = await loaders.finding.load(finding_id)
    vulnerabilities_formatted = await vulns_utils.format_vulnerabilities(
        finding.group_name, loaders, vulnerabilities
    )
    return vulnerabilities_formatted


async def get_vulnerabilities_file(
    loaders: Any, finding_id: str, group_name: str
) -> str:
    vulnerabilities = await get_vulnerabilities_by_type(loaders, finding_id)
    # FP: the generated filename is unpredictable
    file_name = (  # NOSONAR # nosec # noqa: E501
        f"/tmp/{group_name}-{finding_id}_{str(uuid.uuid4())}.yaml"
    )
    with open(  # pylint: disable=unspecified-encoding
        file_name, "w"
    ) as stream:
        yaml.safe_dump(vulnerabilities, stream, default_flow_style=False)

    uploaded_file_url = ""
    with open(file_name, "rb") as bstream:
        uploaded_file = UploadFile(
            filename=bstream.name, content_type="application/yaml"
        )
        await uploaded_file.write(bstream.read())
        await uploaded_file.seek(0)
        uploaded_file_name = await upload_vuln_file_to_s3(uploaded_file)
        uploaded_file_url = await sign_vuln_file_url(uploaded_file_name)
    return uploaded_file_url
