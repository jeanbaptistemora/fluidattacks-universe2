from .dal import (
    sign_vuln_file_url,
    upload_vuln_file_to_s3,
)
from aioextensions import (
    collect,
    schedule,
)
from custom_exceptions import (
    AlreadyZeroRiskConfirmed,
    InvalidCannotModifyNicknameWhenClosing,
    InvalidCommitHash,
    InvalidFileSize,
    InvalidNewVulnState,
    InvalidPath,
    InvalidPort,
    InvalidSchema,
    InvalidSpecific,
    InvalidVulnsNumber,
    MachineCanNotOperate,
    RootNotFound,
)
from dataloaders import (
    Dataloaders,
)
from datetime import (
    datetime,
)
from db_model import (
    vulnerabilities as vulns_model,
)
from db_model.enums import (
    Source,
)
from db_model.findings.enums import (
    FindingStateStatus,
)
from db_model.organization_finding_policies.enums import (
    PolicyStateStatus,
)
from db_model.organization_finding_policies.types import (
    OrgFindingPolicy,
)
from db_model.roots.types import (
    GitRoot,
    RootRequest,
)
from db_model.vulnerabilities.enums import (
    VulnerabilityStateStatus,
    VulnerabilityToolImpact,
    VulnerabilityTreatmentStatus,
    VulnerabilityType,
    VulnerabilityZeroRiskStatus,
)
from db_model.vulnerabilities.types import (
    Vulnerability,
    VulnerabilityMetadataToUpdate,
    VulnerabilityState,
    VulnerabilityTool,
    VulnerabilityTreatment,
    VulnerabilityUnreliableIndicators,
)
from decimal import (
    Decimal,
)
from findings import (
    domain as findings_domain,
)
from graphql.type.definition import (
    GraphQLResolveInfo,
)
import html
from itertools import (
    chain,
)
import json
from machine.availability import (
    operation_can_be_executed,
)
from mailer import (
    findings as findings_mail,
)
from newutils import (
    datetime as datetime_utils,
    files as files_utils,
    logs as logs_utils,
    validations,
    vulnerabilities as vulns_utils,
)
from operator import (
    attrgetter,
)
import os
from pykwalify.core import (
    Core,
)
from pykwalify.errors import (
    CoreError,
    SchemaError,
)
import re
from roots import (
    domain as roots_domain,
)
from sessions import (
    domain as sessions_domain,
)
from starlette.datastructures import (
    UploadFile,
)
from typing import (
    Any,
)
import uuid
from vulnerabilities import (
    domain as vulns_domain,
)
from vulnerabilities.domain.utils import (
    get_hash_from_typed,
)
from vulnerabilities.domain.validations import (
    validate_stream,
)
from vulnerabilities.types import (
    ToolItem,
)
import yaml


def _get_treatment_new_vuln(
    *, modified_date: datetime, finding_policy: OrgFindingPolicy | None
) -> tuple[VulnerabilityTreatment, ...]:
    if (
        finding_policy
        and finding_policy.state.status == PolicyStateStatus.APPROVED
    ):
        return vulns_utils.get_treatment_from_org_finding_policy(
            modified_date=modified_date,
            user_email=finding_policy.state.modified_by,
        )
    return (
        VulnerabilityTreatment(
            modified_date=modified_date,
            status=VulnerabilityTreatmentStatus.UNTREATED,
        ),
    )


async def _add_vulnerability_to_dynamo(
    *,
    vuln_to_add: Vulnerability,
    finding_policy: OrgFindingPolicy | None,
) -> str:
    """Add new vulnerability to DynamoDB."""
    if vuln_to_add.state.status != VulnerabilityStateStatus.VULNERABLE:
        raise InvalidNewVulnState.new()

    new_historic_treatment = _get_treatment_new_vuln(
        modified_date=vuln_to_add.state.modified_date,
        finding_policy=finding_policy,
    )
    new_id = str(uuid.uuid4())
    unreliable_indicators = VulnerabilityUnreliableIndicators(
        unreliable_source=vuln_to_add.state.source,
        unreliable_treatment_changes=vulns_utils.get_treatment_changes(
            new_historic_treatment
        ),
    )
    new_vulnerability = vuln_to_add._replace(
        id=new_id,
        treatment=new_historic_treatment[-1],
        unreliable_indicators=unreliable_indicators,
    )
    await vulns_model.add(vulnerability=new_vulnerability)
    if len(new_historic_treatment) > 1:
        await vulns_model.update_historic(
            current_value=new_vulnerability,
            historic=new_historic_treatment,
        )
    return new_id


def _deduplicate_vulns(
    vulns: list[Vulnerability],
) -> list[Vulnerability]:
    # In case there are repeated vulns, only the latest will be taken into
    # account
    return list(
        {
            get_hash_from_typed(vuln, from_yaml=True): vuln for vuln in vulns
        }.values()
    )


async def _get_vulns_to_add(
    loaders: Dataloaders,
    vulns_data_from_file: dict[str, list[dict[str, Any]]],
    group_name: str,
    finding_id: str,
    user_email: str,
) -> list[Vulnerability]:
    """Get typed vulnerabilities from the data processed from the yaml file."""
    roots = await loaders.group_roots.load(group_name)
    root_ids_by_nicknames = roots_domain.get_root_ids_by_nicknames(
        tuple(roots), only_git_roots=False
    )
    return _deduplicate_vulns(
        list(
            chain.from_iterable(
                map_vulnerability_type(
                    index=index,
                    item=vuln,
                    finding_id=finding_id,
                    group_name=group_name,
                    root_ids_by_nicknames=root_ids_by_nicknames,
                    user_email=user_email,
                    vuln_type=vuln_type,
                )
                for vuln_type in ["inputs", "lines", "ports"]
                for index, vuln in enumerate(
                    vulns_data_from_file.get(vuln_type, [])
                )
            )
        )
    )


def _format_where(vuln_type: str, item: Any, where: str) -> str:
    if vuln_type == "lines":
        if where.startswith(item["repo_nickname"]):
            where.replace(f'{item["repo_nickname"]}/', "")

        # Use Unix-like paths
        if where.find("\\") >= 0:
            path = where.replace("\\", "\\\\")
            raise InvalidPath(expr=f'"values": "{path}"')

    return where


# pylint: disable=too-many-arguments,too-many-locals
def map_vulnerability_type(
    index: int,
    item: dict[str, Any],
    finding_id: str,
    group_name: str,
    root_ids_by_nicknames: dict[str, str],
    user_email: str,
    vuln_type: str,
) -> list[Vulnerability]:
    """Map fields according to vulnerability type and expand data if specific
    is a range or sequence.
    """
    where_headers = {
        "inputs": {"where": "url", "specific": "field"},
        "lines": {"where": "path", "specific": "line"},
        "ports": {"where": "host", "specific": "port"},
    }
    where: str = item[where_headers[vuln_type]["where"]]
    specific: str = item[where_headers[vuln_type]["specific"]]

    where = (
        _format_where(vuln_type, item, where)
        if vuln_type == "lines"
        else where
    )

    commit_hash = str(item["commit_hash"]) if item.get("commit_hash") else None

    stream: str | None = None
    if vuln_type == "inputs":
        stream = str(item["stream"])
        validate_stream(where, stream, index=index, vuln_type=vuln_type)

    if vuln_type == "ports" and not 0 <= int(specific) <= 65535:
        raise InvalidPort(expr=f'"values": "{specific}"')

    validations.validate_sanitized_csv_input(specific)
    vuln_stream = stream.split(",") if stream else None
    tool = (
        VulnerabilityTool(
            name=item["tool"]["name"],
            impact=VulnerabilityToolImpact[
                str(item["tool"]["impact"]).upper()
            ],
        )
        if "tool" in item
        else None
    )
    vulnerabilities: list[Vulnerability] = []
    # Machine does not report vulnerabilities in ranges
    if vuln_type in ("lines", "ports"):
        specific_values = vulns_utils.ungroup_specific(specific)
    else:
        specific_values = (
            [specific]
            if re.match(r"(?P<specific>.*)\s\(.*\)(\s\[.*\])?$", specific)
            else [spec for spec in specific.split(",") if spec]
        )

    today = datetime_utils.get_utc_now()
    for _specific in specific_values:
        # but may use characters like comma `,` in an INPUT specific field
        # Vulnerability id(UUID4) will be replaced later, once the comparison
        # with existing vulns is done
        vulnerabilities = [
            *vulnerabilities,
            Vulnerability(
                created_by=user_email,
                created_date=today,
                finding_id=finding_id,
                group_name=group_name,
                hacker_email=user_email,
                id="",
                state=VulnerabilityState(
                    modified_by=user_email,
                    modified_date=today,
                    source=Source[str(item["source"]).upper()],
                    status=VulnerabilityStateStatus[
                        vulns_utils.get_inverted_state_converted(
                            str(item["state"]).upper()
                        )
                    ],
                    tool=tool,
                    commit=commit_hash,
                    where=where,
                    specific=_specific,
                ),
                type=VulnerabilityType[vuln_type.upper()],
                root_id=roots_domain.get_root_id_by_nicknames(
                    nickname=item["repo_nickname"],
                    root_ids_by_nicknames=root_ids_by_nicknames,
                ),
                skims_method=item.get("skims_method"),
                skims_technique=item.get("skims_technique"),
                hash=item.get("hash") or None,
                developer=item.get("developer"),
                stream=vuln_stream,
                tags=item.get("tags") or None,
            ),
        ]

    return vulnerabilities


def _get_vulns_to_reattack(
    last_status: VulnerabilityStateStatus,
    new_status: VulnerabilityStateStatus,
    vulns_in_db: list[Vulnerability | None],
    vulns_to_add: list[Vulnerability],
) -> list[Vulnerability]:
    return sorted(
        [
            vuln_to_add._replace(id=vuln_in_db.id)
            for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add)
            if (
                vuln_in_db
                and vulns_utils.is_reattack_requested(vuln_in_db)
                and vuln_in_db.state.status == last_status
                and vuln_to_add.state.status == new_status
            )
        ],
        key=attrgetter("id"),
    )


def _get_vulns_ids(vulns: list[Vulnerability]) -> set[str]:
    return set(map(attrgetter("id"), vulns))


async def _validate_nicknames(
    *,
    loaders: Dataloaders,
    group: str,
    vulns_in_db: list[Vulnerability | None],
    vulns_to_add: list[Vulnerability],
) -> None:
    all_roots = await loaders.group_roots.load(group)
    for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add):
        current_root = next(
            (root for root in all_roots if root.id == vuln_to_add.root_id),
            None,
        )
        # You can only report open vulns in an existing root
        if (
            vuln_to_add.state.status == VulnerabilityStateStatus.VULNERABLE
            and not current_root
        ):
            raise RootNotFound()

        if (
            vuln_to_add.type == VulnerabilityType.LINES
            and isinstance(current_root, GitRoot)
            and current_root.cloning.commit
            and vuln_to_add.state.commit != current_root.cloning.commit
            and vuln_to_add.state.source != Source.MACHINE
        ):
            raise InvalidCommitHash()

        # When closing you can set the nickname if it does not exist
        # otherwise it must be equal to the existent nickname
        if vuln_to_add.state.status == VulnerabilityStateStatus.SAFE:
            if vuln_in_db and vuln_in_db.root_id:
                if vuln_in_db.root_id != vuln_to_add.root_id:
                    raise InvalidCannotModifyNicknameWhenClosing.new()
            else:
                if not current_root:
                    raise RootNotFound()


def _sort_vulns_for_comparison(
    vulns: list[Vulnerability],
) -> list[Vulnerability]:
    """Sort vulns, opened first, nickname last"""
    # Open vulns have priority so they can be closed
    # No-Nickname vulns have priority so they can be set their nickname
    return sorted(
        vulns,
        key=lambda vuln: (
            # Open ones first
            vuln.state.status != VulnerabilityStateStatus.VULNERABLE,
            # Nickname last
            bool(vuln.root_id),
        ),
    )


async def map_vulnerabilities_to_dynamo(  # pylint: disable=too-many-locals # noqa: MC0001 # NOSONAR
    *,
    loaders: Dataloaders,
    vulns_data_from_file: dict[str, list[dict[str, Any]]],
    group_name: str,
    finding_id: str,
    finding_policy: OrgFindingPolicy | None,
    context: Any | None = None,
    **kwargs: Any,
) -> set[str]:
    """Map vulnerabilities, send them to DynamoDB, verify them and return the
    processed vulnerabilities."""
    user_info = (
        await sessions_domain.get_jwt_content(context)
        if context
        else kwargs["user_info"]
    )
    user_email = user_info["user_email"]
    # Vulns uploaded by the user
    vulns_to_add: list[Vulnerability] = await _get_vulns_to_add(
        loaders=loaders,
        vulns_data_from_file=vulns_data_from_file,
        group_name=group_name,
        finding_id=finding_id,
        user_email=user_email,
    )

    # Avoid DoS
    if len(vulns_to_add) > 100 and kwargs.get("raise_validation", True):
        raise InvalidVulnsNumber()

    # Vulns as they appear in the DB
    finding_vulns_loader = loaders.finding_vulnerabilities
    finding_vulns = await finding_vulns_loader.load(finding_id)
    sorted_vulns = _sort_vulns_for_comparison(finding_vulns)
    vulns_in_db: list[Vulnerability | None] = [
        next(
            (
                vuln
                for vuln in sorted_vulns
                if get_hash_from_typed(vuln_to_add)
                == get_hash_from_typed(vuln, from_yaml=True)
                and (
                    vuln.state.status == VulnerabilityStateStatus.VULNERABLE
                    or (
                        vuln.state.status == VulnerabilityStateStatus.SAFE
                        and vuln_to_add.state.status
                        == VulnerabilityStateStatus.SAFE
                    )
                )
            ),
            None,
        )
        for vuln_to_add in vulns_to_add
    ]

    # Validate vulnerability roots
    await _validate_nicknames(
        loaders=loaders,
        group=group_name,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    if kwargs.get("raise_validation", True):
        # Validate no uploaded vulns with ZR status
        for vuln_in_db in vulns_in_db:
            if (
                vuln_in_db
                and vuln_in_db.zero_risk is not None
                and vuln_in_db.zero_risk.status
                == VulnerabilityZeroRiskStatus.CONFIRMED
            ):
                raise AlreadyZeroRiskConfirmed(
                    info=(
                        f"Where: {vuln_in_db.state.where}\n"
                        f"Specific: {vuln_in_db.state.specific}"
                    )
                )
    else:
        vulns_in_db = [
            vuln_in_db
            for vuln_in_db in vulns_in_db
            if not (
                vuln_in_db
                and vuln_in_db.zero_risk is not None
                and vuln_in_db.zero_risk.status
                == VulnerabilityZeroRiskStatus.CONFIRMED
            )
        ]

    # Vulnerabilities that have a requested reattack are managed differently
    # They need to have their historic_verification modified so we avoid
    # open vulns being closed in this batch from being pending to reattack
    closed_vulns_to_reattack: list[Vulnerability] = _get_vulns_to_reattack(
        last_status=VulnerabilityStateStatus.VULNERABLE,
        new_status=VulnerabilityStateStatus.SAFE,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    open_vulns_to_reattack: list[Vulnerability] = _get_vulns_to_reattack(
        last_status=VulnerabilityStateStatus.VULNERABLE,
        new_status=VulnerabilityStateStatus.VULNERABLE,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    closed_vulns_ids: set[str] = _get_vulns_ids(closed_vulns_to_reattack)
    open_vulns_ids: set[str] = _get_vulns_ids(open_vulns_to_reattack)

    vulns_to_add_to_dynamo: tuple[Vulnerability | None, ...] = tuple(
        vuln_to_add
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        if not vuln_in_db
        and vuln_to_add.state.status != VulnerabilityStateStatus.SAFE
    )
    # Validate new vulnerabilities exist in the toe (surface)
    vulns_to_add_to_dynamo_list: list[Vulnerability | None] = [
        await vulns_utils.validate_vulnerability_in_toe(
            loaders,
            vuln_to_add,
            index,
            raises=kwargs.get("raise_validation", True),
        )
        for index, vuln_to_add in enumerate(vulns_to_add_to_dynamo)
        if vuln_to_add
    ]

    # Add new vulnerabilities
    added_vuln_ids = set(
        await collect(
            _add_vulnerability_to_dynamo(
                vuln_to_add=vuln_to_add,
                finding_policy=finding_policy,
            )
            for vuln_to_add in tuple(vulns_to_add_to_dynamo_list)
            if vuln_to_add
        )
    )

    # Update those vulns that will not be reattacked
    updated_vuln_ids = set(
        await collect(
            vulns_domain.update_metadata_and_state(
                vulnerability=vuln_in_db,
                new_metadata=VulnerabilityMetadataToUpdate(
                    root_id=vuln_to_add.root_id,
                    stream=vuln_to_add.stream,
                ),
                new_state=vuln_to_add.state,
                finding_policy=finding_policy,
            )
            for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
            # Exclude vulns that we'll verify as those state updates
            # are handled in the verification function
            if vuln_in_db
            and vuln_in_db.id not in closed_vulns_ids
            and vuln_in_db.id not in open_vulns_ids
        )
    )

    if closed_vulns_to_reattack:
        await findings_domain.verify_vulnerabilities(
            context=context,
            finding_id=finding_id,
            user_info=user_info,
            justification=(
                "The vulnerability was re-attacked and found to be closed"
            ),
            open_vulns_ids=[],
            closed_vulns_ids=list(closed_vulns_ids),
            vulns_to_close_from_file=closed_vulns_to_reattack,
            is_reattack_open=False,
            loaders=loaders,
        )

    if open_vulns_to_reattack:
        await findings_domain.verify_vulnerabilities(
            context=context,
            finding_id=finding_id,
            user_info=user_info,
            justification=(
                "The vulnerability was re-attacked and found to be still open"
            ),
            open_vulns_ids=list(open_vulns_ids),
            closed_vulns_ids=[],
            vulns_to_close_from_file=[],
            is_reattack_open=True,
            loaders=loaders,
        )

    # Send mail report
    vulns_properties: dict[str, Any] = await findings_domain.vulns_properties(
        loaders,
        finding_id,
        [vuln for vuln in vulns_to_add_to_dynamo if vuln is not None],
    )
    if vulns_to_add_to_dynamo_list:
        finding = await findings_domain.get_finding(loaders, finding_id)
        severity_score: Decimal = findings_domain.get_severity_score(
            finding.severity
        )
        severity_level: str = findings_domain.get_severity_level(
            severity_score
        )
        if finding.state.status == FindingStateStatus.APPROVED:
            schedule(
                findings_mail.send_mail_vulnerability_report(
                    loaders=loaders,
                    group_name=finding.group_name,
                    finding_title=finding.title,
                    finding_id=finding_id,
                    vulnerabilities_properties=vulns_properties,
                    responsible=finding.hacker_email,
                    severity_score=severity_score,
                    severity_level=severity_level,
                )
            )

    return set.union(
        updated_vuln_ids, closed_vulns_ids, open_vulns_ids, added_vuln_ids
    )


async def process_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: OrgFindingPolicy | None,
    group_name: str,
) -> set[str]:
    """Process a file and return the for vulnerabilities that were
    processed."""
    raw_content = await file_input.read()
    raw_content_decoded = raw_content.decode()
    file_content = html.escape(raw_content_decoded, quote=False)
    await file_input.seek(0)
    vulnerabilities = yaml.safe_load(file_content)
    # FP: the generated filename is unpredictable
    file_url = (  # NOSONAR # nosec # noqa: E501
        f"/tmp/vulnerabilities-{uuid.uuid4()}-{finding_id}.yaml"
    )
    with open(file_url, "w", encoding="utf-8") as stream:
        yaml.safe_dump(vulnerabilities, stream)
    if validate_file_schema(file_url, info):
        return await map_vulnerabilities_to_dynamo(
            context=info.context,
            vulns_data_from_file=vulnerabilities,
            group_name=group_name,
            finding_id=finding_id,
            finding_policy=finding_policy,
            loaders=info.context.loaders,
            raise_validation=True,
        )
    return set()


def ungroup_vulnerability_specific(
    vuln_type: str, specific: str, vulnerability: Vulnerability
) -> list[Vulnerability]:
    """Add vulnerability auxiliar."""
    if vuln_type in ("lines", "ports"):
        specific_values = vulns_utils.ungroup_specific(specific)
    else:
        specific_values = (
            [specific]
            if re.match(r"(?P<specific>.*)\s\(.*\)(\s\[.*\])?$", specific)
            else [spec for spec in specific.split(",") if spec]
        )
    if vuln_type == "ports" and not all(
        (0 <= int(i) <= 65535) for i in specific_values
    ):
        error_value = f'"values": "{specific}"'
        raise InvalidPort(expr=error_value)
    if not specific_values:
        raise InvalidSpecific()
    return [
        vulnerability._replace(
            state=vulnerability.state._replace(specific=specific)
        )
        for specific in specific_values
    ]


async def upload_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: OrgFindingPolicy | None,
    group_name: str,
) -> set[str]:
    """Upload the vulnerabilities and return the vulnerabilities that were
    processed."""
    loaders: Dataloaders = info.context.loaders
    finding = await findings_domain.get_finding(loaders, finding_id)
    if not operation_can_be_executed(info.context, finding.title):
        raise MachineCanNotOperate()

    mib = 1048576
    if await files_utils.get_file_size(file_input) < 1 * mib:
        processed_vulnerabilities = await process_file(
            info,
            file_input,
            finding_id,
            finding_policy,
            group_name,
        )
    else:
        raise InvalidFileSize()
    return processed_vulnerabilities


def validate_file_schema(file_url: str, info: GraphQLResolveInfo) -> bool:
    """Validate if a file has the correct schema."""
    schema_dir = os.path.dirname(os.path.abspath(__file__))
    schema_dir = os.path.join(schema_dir, "vuln_template.yaml")
    core = Core(source_file=file_url, schema_files=[schema_dir])
    is_valid = False
    try:
        core.validate(raise_exception=True)
        is_valid = True
    except SchemaError as ex:
        lines_of_exceptions = core.errors
        errors_values = [
            [
                getattr(x, "pattern", getattr(x, "value", "")),
                getattr(x, "path", getattr(x, "value", "")),
            ]
            for x in lines_of_exceptions
            if not hasattr(x, "key")
        ]
        errors_keys = [x for x in lines_of_exceptions if hasattr(x, "key")]
        errors_values_formated = [json.dumps(x) for x in errors_values]
        errors_keys_formated = [f'"{x.key}, {x.path}"' for x in errors_keys]
        errors_keys_joined = ",".join(errors_keys_formated)
        errors_values_joined = ",".join(errors_values_formated)
        error_value = (
            f'"values": [{errors_values_joined}], '
            f'"keys": [{errors_keys_joined}]'
        )
        logs_utils.cloudwatch_log(
            info.context,
            "Error: An error occurred validating vulnerabilities file",
        )
        raise InvalidSchema(expr=error_value) from ex
    except CoreError as ex:
        raise InvalidSchema() from ex
    finally:
        os.unlink(file_url)
    return is_valid


async def get_vulnerabilities_by_type(
    loaders: Dataloaders, finding_id: str
) -> dict[str, list[dict[str, str | ToolItem]]]:
    """Get vulnerabilities group by type."""
    finding = await findings_domain.get_finding(loaders, finding_id)
    vulnerabilities = await loaders.finding_vulnerabilities_released_nzr.load(
        finding_id
    )
    vulnerabilities_roots = await loaders.root.load_many(
        [
            RootRequest(
                group_name=finding.group_name,
                root_id=vulnerability.root_id or "",
            )
            for vulnerability in vulnerabilities
        ]
    )
    vulnerabilities_formatted = await vulns_utils.format_vulnerabilities(
        vulnerabilities, vulnerabilities_roots
    )

    return vulnerabilities_formatted


async def get_vulnerabilities_file(
    loaders: Dataloaders, finding_id: str, group_name: str
) -> str:
    vulnerabilities = await get_vulnerabilities_by_type(loaders, finding_id)
    # FP: the generated filename is unpredictable
    file_name = (  # NOSONAR # nosec # noqa: E501
        f"/tmp/{group_name}-{finding_id}_{str(uuid.uuid4())}.yaml"
    )
    with open(  # pylint: disable=unspecified-encoding
        file_name, "w"
    ) as stream:
        yaml.safe_dump(vulnerabilities, stream, default_flow_style=False)

    uploaded_file_url = ""
    with open(file_name, "rb") as bstream:
        uploaded_file = UploadFile(
            filename=bstream.name, content_type="application/yaml"
        )
        await uploaded_file.write(bstream.read())
        await uploaded_file.seek(0)
        uploaded_file_name = await upload_vuln_file_to_s3(uploaded_file)
        uploaded_file_url = await sign_vuln_file_url(uploaded_file_name)
    return uploaded_file_url
