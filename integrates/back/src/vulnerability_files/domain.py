from .dal import (
    sign_vuln_file_url,
    upload_vuln_file_to_s3,
)
from aioextensions import (
    collect,
)
from custom_exceptions import (
    ExpectedEscaperField,
    ExpectedPathToStartWithRepo,
    InvalidCannotModifyNicknameWhenClosing,
    InvalidFileSize,
    InvalidNewVulnState,
    InvalidPath,
    InvalidPort,
    InvalidSchema,
    InvalidSpecific,
    InvalidVulnsNumber,
    MachineCanNotOperate,
    RootNotFound,
    ToeInputNotFound,
    ToeLinesNotFound,
    VulnerabilityPathDoesNotExistInToeLines,
    VulnerabilityUrlFieldDoNotExistInToeInputs,
)
from db_model import (
    vulnerabilities as vulns_model,
)
from db_model.enums import (
    Source,
)
from db_model.findings.types import (
    Finding,
)
from db_model.roots.enums import (
    RootStatus,
)
from db_model.roots.types import (
    Root,
)
from db_model.toe_inputs.types import (
    ToeInputRequest,
)
from db_model.toe_lines.types import (
    ToeLinesRequest,
)
from db_model.vulnerabilities.enums import (
    VulnerabilityStateStatus,
    VulnerabilityTreatmentStatus,
    VulnerabilityType,
)
from db_model.vulnerabilities.types import (
    Vulnerability,
    VulnerabilityMetadataToUpdate,
    VulnerabilityState,
    VulnerabilityTreatment,
    VulnerabilityUnreliableIndicators,
)
from dynamodb.types import (
    OrgFindingPolicyItem,
)
from findings import (
    domain as findings_domain,
)
from graphql.type.definition import (
    GraphQLResolveInfo,
)
import html
from itertools import (
    chain,
)
import json
from machine.availability import (
    operation_can_be_executed,
)
from newutils import (
    datetime as datetime_utils,
    files as files_utils,
    logs as logs_utils,
    requests as requests_utils,
    token as token_utils,
    vulnerabilities as vulns_utils,
)
from operator import (
    attrgetter,
)
import os
from pykwalify.core import (
    Core,
)
from pykwalify.errors import (
    CoreError,
    SchemaError,
)
import re
from roots import (
    domain as roots_domain,
)
from starlette.datastructures import (
    UploadFile,
)
from typing import (
    Any,
    cast,
    Dict,
    List,
    Optional,
    Set,
    Tuple,
)
import uuid
from vulnerabilities import (
    domain as vulns_domain,
)
from vulnerabilities.domain.utils import (
    get_hash_from_dict,
    get_hash_from_typed,
)
from vulnerabilities.domain.validations import (
    validate_stream,
)
import yaml  # type: ignore


def _get_treatment_new_vuln(
    *, modified_date: str, finding_policy: Optional[OrgFindingPolicyItem]
) -> Tuple[VulnerabilityTreatment, ...]:
    if finding_policy and finding_policy.state.status == "APPROVED":
        return vulns_utils.get_treatment_from_org_finding_policy(
            modified_date=modified_date,
            user_email=finding_policy.state.modified_by,
        )
    return (
        VulnerabilityTreatment(
            modified_date=modified_date,
            status=VulnerabilityTreatmentStatus.NEW,
        ),
    )


async def _add_vulnerability_to_dynamo(
    *,
    vuln_to_add: Vulnerability,
    finding_policy: Optional[OrgFindingPolicyItem],
) -> str:
    """Add new vulnerability to DynamoDB."""
    if vuln_to_add.state.status != VulnerabilityStateStatus.OPEN:
        raise InvalidNewVulnState.new()

    new_historic_treatment = _get_treatment_new_vuln(
        modified_date=vuln_to_add.state.modified_date,
        finding_policy=finding_policy,
    )
    new_id = str(uuid.uuid4())
    unreliable_indicators = VulnerabilityUnreliableIndicators(
        unreliable_report_date=vuln_to_add.state.modified_date,
        unreliable_source=vuln_to_add.state.source,
        unreliable_treatment_changes=vulns_utils.get_treatment_changes(
            new_historic_treatment
        ),
    )
    new_vulnerability = vuln_to_add._replace(
        id=new_id,
        treatment=new_historic_treatment[-1],
        unreliable_indicators=unreliable_indicators,
    )
    await vulns_model.add(vulnerability=new_vulnerability)
    if len(new_historic_treatment) > 1:
        await vulns_model.update_historic(
            current_value=new_vulnerability,
            historic=new_historic_treatment,
        )
    return new_id


def _deduplicate_vulns(vulns: List[Dict[str, str]]) -> List[Dict[str, str]]:
    # In case there are repeated vulns, only the latest will be taken into
    # account
    return list({get_hash_from_dict(vuln): vuln for vuln in vulns}.values())


def _format_vuln_data_as_typed(  # pylint: disable=too-many-arguments
    root_ids_by_nicknames: Dict[str, str],
    vuln_data: Dict[str, str],
    finding_id: str,
    source: Source,
    today: str,
    user_email: str,
) -> Vulnerability:
    modified_by = (
        user_email
        if not vuln_data.get("analyst", "")
        else vuln_data["analyst"]
    )
    vuln_source = (
        source
        if not vuln_data.get("source", "")
        else Source[vuln_data["source"].upper()]
    )
    vuln_stream = (
        vuln_data["stream"].split(",") if vuln_data.get("stream") else None
    )
    # Vulnerability id(UUID4) will be replaced later, once the comparison with
    # existing vulns is done
    return Vulnerability(
        finding_id=finding_id,
        id="",
        specific=vuln_data["specific"],
        state=VulnerabilityState(
            modified_by=modified_by,
            modified_date=today,
            source=vuln_source,
            status=VulnerabilityStateStatus[vuln_data["state"].upper()],
        ),
        type=VulnerabilityType[vuln_data["vuln_type"].upper()],
        where=vuln_data["where"],
        repo=vuln_data.get("repo_nickname", None),
        root_id=None
        if vuln_data.get("repo_nickname") is None
        else roots_domain.get_root_id_by_nicknames(
            nickname=vuln_data["repo_nickname"],
            root_ids_by_nicknames=root_ids_by_nicknames,
        ),
        skims_method=vuln_data.get("skims_method", None),
        skims_technique=vuln_data.get("skims_technique", None),
        developer=vuln_data.get("developer", None),
        commit=vuln_data.get("commit_hash", None),
        stream=vuln_stream,
    )


async def _get_vulns_to_add(  # pylint: disable=too-many-arguments
    loaders: Any,
    vulns_data_from_file: Dict[str, List[Dict[str, str]]],
    group_name: str,
    finding_id: str,
    source: Source,
    today: str,
    user_email: str,
) -> List[Vulnerability]:
    """Get typed vulnerabilities from the data processed from the yaml file."""
    vulns_data_mapped = _deduplicate_vulns(
        list(
            chain.from_iterable(
                map_vulnerability_type(
                    vuln,
                    vuln_type,
                )
                for vuln_type in ["inputs", "lines", "ports"]
                for vuln in vulns_data_from_file.get(vuln_type, [])
            )
        )
    )
    roots: Tuple[Root, ...] = await loaders.group_roots.load(group_name)
    root_ids_by_nicknames = roots_domain.get_root_ids_by_nicknames(
        roots, only_git_roots=False
    )
    return [
        _format_vuln_data_as_typed(
            root_ids_by_nicknames=root_ids_by_nicknames,
            vuln_data=vuln_data,
            finding_id=finding_id,
            source=source,
            today=today,
            user_email=user_email,
        )
        for vuln_data in vulns_data_mapped
    ]


def map_line_vulnerability(item: Dict[str, str], where: str) -> Dict[str, str]:
    line_data = {
        "where": where,
    }

    # Propagate the commit_hash
    line_data["commit_hash"] = item["commit_hash"]

    # Where must start with repo_nickname
    if not line_data["where"].startswith(item["repo_nickname"]):
        raise ExpectedPathToStartWithRepo.new()

    # Use Unix-like paths
    if line_data["where"].find("\\") >= 0:
        path = line_data["where"].replace("\\", "\\\\")
        raise InvalidPath(expr=f'"values": "{path}"')

    return line_data


def map_vulnerability_type(  # noqa: MC0001
    item: Dict[str, str],
    vuln_type: str,
) -> List[Dict[str, str]]:
    """Map fields according to vulnerability type and expand data if specific
    is a range or sequence.
    """
    where_headers = {
        "inputs": {"where": "url", "specific": "field"},
        "lines": {"where": "path", "specific": "line"},
        "ports": {"where": "host", "specific": "port"},
    }

    where = item[where_headers[vuln_type]["where"]]

    data: Dict[str, str] = {
        "state": item["state"],
        "vuln_type": vuln_type,
        "where": where,
        "repo_nickname": item["repo_nickname"],
    }

    # Validate source and escaper field
    if item.get("source"):
        data["source"] = item["source"]
        if item["source"] == "escape":
            if "escaper" not in item:
                raise ExpectedEscaperField.new()
            data["analyst"] = item["escaper"]

    specific: str = item[where_headers[vuln_type]["specific"]]

    # Propagate skims_method if present
    if skims_method := item.get("skims_method"):
        data["skims_method"] = skims_method

    # Propagate skims_technique if present
    if skims_technique := item.get("skims_technique"):
        data["skims_technique"] = skims_technique

    # Propagate developer if present
    if developer := item.get("developer"):
        data["developer"] = developer

    if vuln_type == "lines":
        line_data = map_line_vulnerability(item, where)
        data.update(line_data)

    elif vuln_type == "inputs":
        # Propagate the stream
        validate_stream(data["where"], item["stream"])
        data["stream"] = item["stream"]

    # Nothing custom for now
    # elif vuln_type == "ports":
    #     pass

    if vulns_utils.is_range(specific) or vulns_utils.is_sequence(specific):
        return ungroup_vulnerability_specific(vuln_type, specific, data)

    if vuln_type == "ports" and not 0 <= int(specific) <= 65535:
        raise InvalidPort(expr=f'"values": "{specific}"')

    return [{**data, "specific": specific}]


def _get_vulns_to_reattack(
    last_status: VulnerabilityStateStatus,
    new_status: VulnerabilityStateStatus,
    vulns_in_db: List[Optional[Vulnerability]],
    vulns_to_add: List[Vulnerability],
) -> List[Vulnerability]:
    return sorted(
        [
            vuln_to_add._replace(id=vuln_in_db.id)
            for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add)
            if (
                vuln_in_db
                and vulns_utils.is_reattack_requested(vuln_in_db)
                and vuln_in_db.state.status == last_status
                and vuln_to_add.state.status == new_status
            )
        ],
        key=attrgetter("id"),
    )


def _get_vulns_ids(vulns: List[Vulnerability]) -> Set[str]:
    return set(map(attrgetter("id"), vulns))


async def _validate_nicknames(
    *,
    loaders: Any,
    group: str,
    vulns_in_db: List[Optional[Vulnerability]],
    vulns_to_add: List[Vulnerability],
) -> None:
    nicknames: Set[str] = {
        root.state.nickname
        for root in await loaders.group_roots.load(group)
        if root.state.status == RootStatus.ACTIVE
    }
    for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add):
        # You can only report open vulns in an existing root
        if (
            vuln_to_add.state.status == VulnerabilityStateStatus.OPEN
            and vuln_to_add.repo not in nicknames
        ):
            raise RootNotFound()
        # When closing you can set the nickname if it does not exist
        # otherwise it must be equal to the existent nickname
        if vuln_to_add.state.status == VulnerabilityStateStatus.CLOSED:
            if vuln_in_db and vuln_in_db.repo:
                if vuln_in_db.repo != vuln_to_add.repo:
                    raise InvalidCannotModifyNicknameWhenClosing.new()
            else:
                if vuln_to_add.repo not in nicknames:
                    raise RootNotFound()


def _sort_vulns_for_comparison(
    vulns: List[Vulnerability],
) -> List[Vulnerability]:
    """Sort vulns, opened first, nickname last"""
    # Open vulns have priority so they can be closed
    # No-Nickname vulns have priority so they can be set their nickname
    return sorted(
        vulns,
        key=lambda vuln: (
            # Open ones first
            vuln.state.status != VulnerabilityStateStatus.OPEN,
            # Nickname last
            bool(vuln.repo),
        ),
    )


async def _map_vulnerabilities_to_dynamo(  # pylint: disable=too-many-locals
    *,
    context: Any,
    vulns_data_from_file: Dict[str, List[Dict[str, str]]],
    group_name: str,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
) -> Set[str]:
    """Map vulnerabilities, send them to DynamoDB, verify them and return the
    processed vulnerabilities."""
    source = requests_utils.get_source_new(context)
    user_info = await token_utils.get_jwt_content(context)
    user_email = user_info["user_email"]
    today = datetime_utils.get_iso_date()
    # Vulns uploaded by the user
    vulns_to_add: List[Vulnerability] = await _get_vulns_to_add(
        loaders=context.loaders,
        vulns_data_from_file=vulns_data_from_file,
        group_name=group_name,
        finding_id=finding_id,
        source=source,
        today=today,
        user_email=user_email,
    )

    # Avoid DoS
    if len(vulns_to_add) > 100:
        raise InvalidVulnsNumber()

    # Vulns as they appear in the DB
    finding_vulns_loader = context.loaders.finding_vulnerabilities
    finding_vulns = await finding_vulns_loader.load(finding_id)
    sorted_vulns = _sort_vulns_for_comparison(finding_vulns)
    vulns_in_db: List[Optional[Vulnerability]] = [
        next(
            (
                vuln
                for vuln in sorted_vulns
                if get_hash_from_typed(vuln_to_add)
                == get_hash_from_typed(vuln, from_yaml=True)
                and (
                    vuln.state.status == VulnerabilityStateStatus.OPEN
                    or (
                        vuln.state.status == VulnerabilityStateStatus.CLOSED
                        and vuln_to_add.state.status
                        == VulnerabilityStateStatus.CLOSED
                    )
                )
            ),
            None,
        )
        for vuln_to_add in vulns_to_add
    ]

    # Validate vulnerability roots
    await _validate_nicknames(
        loaders=context.loaders,
        group=group_name,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )

    # Vulnerabilities that have a requested reattack are managed differently
    # They need to have their historic_verification modified so we avoid
    # open vulns being closed in this batch from being pending to reattack
    closed_vulns_to_reattack: List[Vulnerability] = _get_vulns_to_reattack(
        last_status=VulnerabilityStateStatus.OPEN,
        new_status=VulnerabilityStateStatus.CLOSED,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    open_vulns_to_reattack: List[Vulnerability] = _get_vulns_to_reattack(
        last_status=VulnerabilityStateStatus.OPEN,
        new_status=VulnerabilityStateStatus.OPEN,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    closed_vulns_ids: Set[str] = _get_vulns_ids(closed_vulns_to_reattack)
    open_vulns_ids: Set[str] = _get_vulns_ids(open_vulns_to_reattack)

    vulns_to_add_to_dynamo = tuple(
        vuln_to_add
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        if not vuln_in_db
    )
    # Validate new vulnerabilities exist in the toe (surface)
    await collect(
        tuple(
            validate_vulnerability_in_toe(
                context.loaders, group_name, vuln_to_add
            )
            for vuln_to_add in vulns_to_add_to_dynamo
        )
    )
    # Add new vulnerabilities
    await collect(
        _add_vulnerability_to_dynamo(
            vuln_to_add=vuln_to_add,
            finding_policy=finding_policy,
        )
        for vuln_to_add in vulns_to_add_to_dynamo
    )

    # Update those vulns that will not be reattacked
    updated_vuln_ids = set(
        await collect(
            vulns_domain.update_metadata_and_state(
                vulnerability=vuln_in_db,
                new_metadata=VulnerabilityMetadataToUpdate(
                    commit=vuln_to_add.commit,
                    repo=vuln_to_add.repo,
                    stream=vuln_to_add.stream,
                ),
                new_state=vuln_to_add.state,
                finding_policy=finding_policy,
            )
            for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
            # Exclude vulns that we'll verify as those state updates
            # are handled in the verification function
            if vuln_in_db
            and vuln_in_db.id not in closed_vulns_ids
            and vuln_in_db.id not in open_vulns_ids
        )
    )

    if closed_vulns_to_reattack:
        await findings_domain.verify_vulnerabilities(
            context=context,
            finding_id=finding_id,
            user_info=user_info,
            justification=(
                "The vulnerability was re-attacked and found to be closed"
            ),
            open_vulns_ids=[],
            closed_vulns_ids=list(closed_vulns_ids),
            vulns_to_close_from_file=closed_vulns_to_reattack,
            is_reattack_open=False,
        )

    if open_vulns_to_reattack:
        await findings_domain.verify_vulnerabilities(
            context=context,
            finding_id=finding_id,
            user_info=user_info,
            justification=(
                "The vulnerability was re-attacked and found to be still open"
            ),
            open_vulns_ids=list(open_vulns_ids),
            closed_vulns_ids=[],
            vulns_to_close_from_file=[],
            is_reattack_open=True,
        )

    return set.union(updated_vuln_ids, closed_vulns_ids, open_vulns_ids)


async def process_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> Set[str]:
    """Process a file and return the for vulnerabilities that were
    processed."""
    raw_content = await file_input.read()
    raw_content = cast(bytes, raw_content).decode()
    file_content = html.escape(raw_content, quote=False)
    await file_input.seek(0)
    vulnerabilities = yaml.safe_load(file_content)
    # FP: the generated filename is unpredictable
    file_url = (  # NOSONAR # nosec # noqa: E501
        f"/tmp/vulnerabilities-{uuid.uuid4()}-{finding_id}.yaml"
    )
    with open(file_url, "w", encoding="utf-8") as stream:
        yaml.safe_dump(vulnerabilities, stream)
    if validate_file_schema(file_url, info):
        return await _map_vulnerabilities_to_dynamo(
            context=info.context,
            vulns_data_from_file=vulnerabilities,
            group_name=group_name,
            finding_id=finding_id,
            finding_policy=finding_policy,
        )
    return set()


def ungroup_vulnerability_specific(
    vuln_type: str, specific: str, data: Dict[str, str]
) -> List[Dict[str, str]]:
    """Add vulnerability auxiliar."""
    if vuln_type in ("lines", "ports"):
        specific_values = vulns_utils.ungroup_specific(specific)
    else:
        specific_values = [spec for spec in specific.split(",") if spec]
    if vuln_type == "ports" and not all(
        (0 <= int(i) <= 65535) for i in specific_values
    ):
        error_value = f'"values": "{specific}"'
        raise InvalidPort(expr=error_value)
    if not specific_values:
        raise InvalidSpecific()
    return [{**data, "specific": specific} for specific in specific_values]


async def upload_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> Set[str]:
    """Upload the vulnerabilities and return the vulnerabilities that were
    processed."""
    finding_loader = info.context.loaders.finding
    finding: Finding = await finding_loader.load(finding_id)
    if not operation_can_be_executed(info.context, finding.title):
        raise MachineCanNotOperate()

    mib = 1048576
    if await files_utils.get_file_size(file_input) < 1 * mib:
        processed_vulnerabilities = await process_file(
            info,
            file_input,
            finding_id,
            finding_policy,
            group_name,
        )
    else:
        raise InvalidFileSize()
    return processed_vulnerabilities


def validate_file_schema(file_url: str, info: GraphQLResolveInfo) -> bool:
    """Validate if a file has the correct schema."""
    schema_dir = os.path.dirname(os.path.abspath(__file__))
    schema_dir = os.path.join(schema_dir, "vuln_template.yaml")
    core = Core(source_file=file_url, schema_files=[schema_dir])
    is_valid = False
    try:
        core.validate(raise_exception=True)
        is_valid = True
    except SchemaError as ex:
        lines_of_exceptions = core.errors
        errors_values = [
            getattr(x, "pattern", "")
            for x in lines_of_exceptions
            if not hasattr(x, "key")
        ]
        errors_keys = [x for x in lines_of_exceptions if hasattr(x, "key")]
        errors_values_formated = [json.dumps(x) for x in errors_values]
        errors_keys_formated = [f'"{x.key}"' for x in errors_keys]
        errors_keys_joined = ",".join(errors_keys_formated)
        errors_values_joined = ",".join(errors_values_formated)
        error_value = (
            f'"values": [{errors_values_joined}], '
            f'"keys": [{errors_keys_joined}]'
        )
        logs_utils.cloudwatch_log(
            info.context,
            "Error: An error occurred validating vulnerabilities file",
        )
        raise InvalidSchema(expr=error_value) from ex
    except CoreError as ex:
        raise InvalidSchema() from ex
    finally:
        os.unlink(file_url)
    return is_valid


async def validate_vulnerability_in_toe(
    loaders: Any, group_name: str, vulnerability: Vulnerability
) -> None:
    if vulnerability.root_id:
        where = vulnerability.where
        # There are cases, like SCA vulns, where the `where` attribute
        # has additional information `filename (pacakge) [CVE]`
        if match := re.match(r"(?P<where>.*)\s\(.*\)\s\[.*\]", where):
            where = match.groupdict()["where"]

        if vulnerability.type == VulnerabilityType.LINES:
            slash_index = where.find("/")
            if slash_index == -1:
                raise VulnerabilityPathDoesNotExistInToeLines()
            filename = where[slash_index + 1 :]
            try:
                await loaders.toe_lines.load(
                    ToeLinesRequest(
                        filename=filename,
                        group_name=group_name,
                        root_id=vulnerability.root_id,
                    )
                )
            except ToeLinesNotFound as exc:
                raise VulnerabilityPathDoesNotExistInToeLines() from exc

        if vulnerability.type == VulnerabilityType.INPUTS:
            try:
                await loaders.toe_input.load(
                    ToeInputRequest(
                        component=where,
                        entry_point=vulnerability.specific,
                        group_name=group_name,
                        root_id=vulnerability.root_id,
                    )
                )
            except ToeInputNotFound as exc:
                raise VulnerabilityUrlFieldDoNotExistInToeInputs() from exc


async def get_vulnerabilities_by_type(
    loaders: Any, finding_id: str
) -> Dict[str, List[Dict[str, str]]]:
    """Get vulnerabilities group by type."""
    vulnerabilities = await loaders.finding_vulnerabilities_nzr.load(
        finding_id
    )
    vulnerabilities_formatted = vulns_utils.format_vulnerabilities(
        vulnerabilities
    )
    return vulnerabilities_formatted


async def get_vulnerabilities_file(
    loaders: Any, finding_id: str, group_name: str
) -> str:
    vulnerabilities = await get_vulnerabilities_by_type(loaders, finding_id)
    # FP: the generated filename is unpredictable
    file_name = (  # NOSONAR # nosec # noqa: E501
        f"/tmp/{group_name}-{finding_id}_{str(uuid.uuid4())}.yaml"
    )
    with open(  # pylint: disable=unspecified-encoding
        file_name, "w"
    ) as stream:
        yaml.safe_dump(vulnerabilities, stream, default_flow_style=False)

    uploaded_file_url = ""
    with open(file_name, "rb") as bstream:
        uploaded_file = UploadFile(
            filename=bstream.name, content_type="application/yaml"
        )
        await uploaded_file.write(bstream.read())
        await uploaded_file.seek(0)
        uploaded_file_name = await upload_vuln_file_to_s3(uploaded_file)
        uploaded_file_url = await sign_vuln_file_url(uploaded_file_name)
    return uploaded_file_url
