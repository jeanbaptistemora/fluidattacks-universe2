from aioextensions import (
    collect,
)
from back.src.machine.availability import (
    operation_can_be_executed,
)
from custom_exceptions import (
    ExpectedEscaperField,
    ExpectedPathToStartWithRepo,
    ExpectedVulnToHaveNickname,
    InvalidCannotModifyNicknameWhenClosing,
    InvalidFileSize,
    InvalidNewVulnState,
    InvalidPath,
    InvalidPort,
    InvalidSchema,
    InvalidSpecific,
    InvalidVulnsNumber,
    MachineCanNotOperate,
    RootNotFound,
)
from custom_types import (
    User as UserType,
)
from db_model.enums import (
    Source,
)
from db_model.findings.types import (
    Finding,
)
from db_model.vulnerabilities.enums import (
    VulnerabilityStateStatus,
    VulnerabilityTreatmentStatus,
    VulnerabilityType,
)
from db_model.vulnerabilities.types import (
    Vulnerability,
    VulnerabilityMetadataToUpdate,
    VulnerabilityState,
    VulnerabilityTreatment,
)
from dynamodb.types import (
    OrgFindingPolicyItem,
)
from findings import (
    domain as findings_domain,
)
from graphql.type.definition import (
    GraphQLResolveInfo,
)
import html
from itertools import (
    chain,
)
import json
from newutils import (
    datetime as datetime_utils,
    files as files_utils,
    logs as logs_utils,
    requests as requests_utils,
    token as token_utils,
    vulnerabilities as vulns_utils,
)
from operator import (
    attrgetter,
)
import os
from pykwalify.core import (
    Core,
)
from pykwalify.errors import (
    CoreError,
    SchemaError,
)
from starlette.datastructures import (
    UploadFile,
)
from typing import (
    Any,
    Awaitable,
    cast,
    Dict,
    List,
    Optional,
    Set,
    Tuple,
)
import uuid
from vulnerabilities import (
    dal as vulns_dal,
    domain as vulns_domain,
)
from vulnerabilities.domain.utils import (
    get_hash_from_dict,
    get_hash_from_typed,
)
from vulnerabilities.domain.validations import (
    validate_stream,
)
import yaml  # type: ignore


def _get_treatment_new_vuln(
    *, modified_date: str, finding_policy: Optional[OrgFindingPolicyItem]
) -> Tuple[VulnerabilityTreatment, ...]:
    if finding_policy and finding_policy.state.status == "APPROVED":
        return vulns_utils.get_treatment_from_org_finding_policy(
            modified_date=modified_date,
            user_email=finding_policy.state.modified_by,
        )
    return (
        VulnerabilityTreatment(
            modified_date=modified_date,
            status=VulnerabilityTreatmentStatus.NEW,
        ),
    )


async def _add_vulnerability_to_dynamo(
    *,
    vulnerability: Optional[Vulnerability],
    to_update: Vulnerability,
    finding_policy: Optional[OrgFindingPolicyItem],
) -> bool:
    """Add or update vulnerability in DynamoDB."""
    if vulnerability:
        # Update metadata and historics for vuln already in db
        return await vulns_domain.update_metadata_and_state(
            vulnerability=vulnerability,
            new_metadata=VulnerabilityMetadataToUpdate(
                commit=to_update.commit,
                repo=to_update.repo,
                stream=to_update.stream,
            ),
            new_state=to_update.state,
            finding_policy=finding_policy,
        )

    if to_update.state.status != VulnerabilityStateStatus.OPEN:
        raise InvalidNewVulnState.new()

    new_vulnerability = to_update._replace(id=str(uuid.uuid4()))
    new_historic_treatment = _get_treatment_new_vuln(
        modified_date=to_update.state.modified_date,
        finding_policy=finding_policy,
    )
    return await vulns_dal.create_new(
        vulnerability=new_vulnerability,
        historic_treatment=new_historic_treatment,
    )


def _deduplicate_vulns(vulns: List[Dict[str, str]]) -> List[Dict[str, str]]:
    # In case there are repeated vulns, only the latest will be taken into
    # account
    return list({get_hash_from_dict(vuln): vuln for vuln in vulns}.values())


def _format_vuln_data_as_typed(
    vuln_data: Dict[str, str],
    finding_id: str,
    source: Source,
    today: str,
    user_email: str,
) -> Vulnerability:
    modified_by = (
        user_email
        if not vuln_data.get("analyst", "")
        else vuln_data["analyst"]
    )
    vuln_source = (
        source
        if not vuln_data.get("source", "")
        else Source[vuln_data["source"].upper()]
    )
    vuln_stream = (
        vuln_data["stream"].split(",") if vuln_data.get("stream") else None
    )
    # Vulnerability id(UUID4) will be replaced later, once the comparison with
    # existing vulns is done
    return Vulnerability(
        finding_id=finding_id,
        id="",
        specific=vuln_data["specific"],
        state=VulnerabilityState(
            modified_by=modified_by,
            modified_date=today,
            source=vuln_source,
            status=VulnerabilityStateStatus[vuln_data["state"].upper()],
        ),
        type=VulnerabilityType[vuln_data["vuln_type"].upper()],
        where=vuln_data["where"],
        repo=vuln_data.get("repo_nickname", None),
        commit=vuln_data.get("commit_hash", None),
        stream=vuln_stream,
    )


def _get_vulns_to_add(
    vulns_data_from_file: Dict[str, List[Dict[str, str]]],
    finding_id: str,
    source: Source,
    today: str,
    user_email: str,
) -> List[Vulnerability]:
    """Get typed vulnerabilities from the data processed from the yaml file."""
    vulns_data_mapped = _deduplicate_vulns(
        list(
            chain.from_iterable(
                map_vulnerability_type(
                    vuln,
                    vuln_type,
                )
                for vuln_type in ["inputs", "lines", "ports"]
                for vuln in vulns_data_from_file.get(vuln_type, [])
            )
        )
    )
    return [
        _format_vuln_data_as_typed(
            vuln_data=vuln_data,
            finding_id=finding_id,
            source=source,
            today=today,
            user_email=user_email,
        )
        for vuln_data in vulns_data_mapped
    ]


def map_vulnerability_type(  # noqa: MC0001
    item: Dict[str, str],
    vuln_type: str,
) -> List[Dict[str, str]]:
    """Map fields according to vulnerability type and expand data if specific
    is a range or sequence.
    """
    response = []
    where_headers = {
        "inputs": {"where": "url", "specific": "field"},
        "lines": {"where": "path", "specific": "line"},
        "ports": {"where": "host", "specific": "port"},
    }

    data: Dict[str, str] = {
        "state": item["state"],
        "vuln_type": vuln_type,
        "where": item[where_headers[vuln_type]["where"]],
    }

    # Validate source and escaper field
    if item.get("source"):
        data["source"] = item["source"]
        if item["source"] == "escape" and "escaper" not in item:
            raise ExpectedEscaperField.new()
        data["analyst"] = item["escaper"]

    specific: str = item[where_headers[vuln_type]["specific"]]
    if "repo_nickname" in item:
        data["repo_nickname"] = item["repo_nickname"]
    else:
        raise ExpectedVulnToHaveNickname.new()

    if vuln_type == "lines":
        # Propagate the commit_hash
        data["commit_hash"] = item["commit_hash"]

        # Where must start with repo_nickname
        if not data["where"].startswith(data["repo_nickname"]):
            raise ExpectedPathToStartWithRepo.new()

        # Use Unix-like paths
        if data["where"].find("\\") >= 0:
            path = data["where"].replace("\\", "\\\\")
            raise InvalidPath(expr=f'"values": "{path}"')

    elif vuln_type == "inputs":
        # Propagate the stream
        validate_stream(data["where"], item["stream"])
        data["stream"] = item["stream"]

    elif vuln_type == "ports":
        # Nothing custom for now
        pass

    if vulns_utils.is_range(specific) or vulns_utils.is_sequence(specific):
        response.extend(
            ungroup_vulnerability_specific(vuln_type, specific, data)
        )
    else:
        if vuln_type == "ports" and not 0 <= int(specific) <= 65535:
            raise InvalidPort(expr=f'"values": "{specific}"')
        response.append({**data, "specific": specific})

    return response


def _get_vulns_to_reattack(
    last_status: VulnerabilityStateStatus,
    new_status: VulnerabilityStateStatus,
    vulns_in_db: List[Optional[Vulnerability]],
    vulns_to_add: List[Vulnerability],
) -> List[Vulnerability]:
    return sorted(
        [
            vuln_to_add._replace(id=vuln_in_db.id)
            for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add)
            if (
                vuln_in_db
                and vulns_utils.is_reattack_requested(vuln_in_db)
                and vuln_in_db.state.status == last_status
                and vuln_to_add.state.status == new_status
            )
        ],
        key=attrgetter("id"),
    )


def _get_vulns_ids(vulns: List[Vulnerability]) -> Set[str]:
    return set(map(attrgetter("id"), vulns))


async def _validate_nicknames(
    *,
    loaders: Any,
    group: str,
    vulns_in_db: List[Optional[Vulnerability]],
    vulns_to_add: List[Vulnerability],
) -> None:
    nicknames: Set[str] = {
        root.state.nickname
        for root in await loaders.group_roots.load(group)
        if root.state.status == "ACTIVE"
    }
    for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add):
        # You can only report open vulns in an existing root
        if (
            vuln_to_add.state.status == VulnerabilityStateStatus.OPEN
            and vuln_to_add.repo not in nicknames
        ):
            raise RootNotFound()
        # When closing you can set the nickname if it does not exist
        # otherwise it must be equal to the existent nickname
        if vuln_to_add.state.status == VulnerabilityStateStatus.CLOSED:
            if vuln_in_db and vuln_in_db.repo:
                if vuln_in_db.repo != vuln_to_add.repo:
                    raise InvalidCannotModifyNicknameWhenClosing.new()
            else:
                if vuln_to_add.repo not in nicknames:
                    raise RootNotFound()


def _sort_vulns_for_comparison(
    vulns: List[Vulnerability],
) -> List[Vulnerability]:
    """Sort vulns, opened first, nickname last"""
    # Open vulns have priority so they can be closed
    # No-Nickname vulns have priority so they can be set their nickname
    return sorted(
        vulns,
        key=lambda vuln: (
            # Open ones first
            vuln.state.status != VulnerabilityStateStatus.OPEN,
            # Nickname last
            bool(vuln.repo),
        ),
    )


async def _map_vulnerabilities_to_dynamo(  # pylint: disable=too-many-locals
    *,
    context: Any,
    vulns_data_from_file: Dict[str, List[Dict[str, str]]],
    group_name: str,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
) -> bool:
    """Map vulnerabilities and send them to DynamoDB."""
    source = requests_utils.get_source_new(context)
    user_info: UserType = await token_utils.get_jwt_content(context)
    user_email: str = user_info["user_email"]
    today = datetime_utils.get_iso_date()
    # Vulns uploaded by the user
    vulns_to_add: List[Vulnerability] = _get_vulns_to_add(
        vulns_data_from_file=vulns_data_from_file,
        finding_id=finding_id,
        source=source,
        today=today,
        user_email=user_email,
    )

    # Avoid DoS
    if len(vulns_to_add) > 100:
        raise InvalidVulnsNumber()

    # Vulns as they appear in the DB
    finding_vulns_loader = context.loaders.finding_vulns_typed
    finding_vulns = await finding_vulns_loader.load(finding_id)
    sorted_vulns = _sort_vulns_for_comparison(finding_vulns)
    vulns_in_db: List[Optional[Vulnerability]] = [
        next(
            (
                vuln
                for vuln in sorted_vulns
                if get_hash_from_typed(vuln_to_add)
                == get_hash_from_typed(vuln, from_yaml=True)
                and (
                    vuln.state.status == VulnerabilityStateStatus.OPEN
                    or (
                        vuln.state.status == VulnerabilityStateStatus.CLOSED
                        and vuln_to_add.state.status
                        == VulnerabilityStateStatus.CLOSED
                    )
                )
            ),
            None,
        )
        for vuln_to_add in vulns_to_add
    ]

    # Validate vulnerability roots
    await _validate_nicknames(
        loaders=context.loaders,
        group=group_name,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )

    # Vulnerabilities that have a requested reattack are managed differently
    # They need to have their historic_verification modified so we avoid
    # open vulns being closed in this batch from being pending to reattack
    closed_vulns_to_reattack: List[Vulnerability] = _get_vulns_to_reattack(
        last_status=VulnerabilityStateStatus.OPEN,
        new_status=VulnerabilityStateStatus.CLOSED,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    open_vulns_to_reattack: List[Vulnerability] = _get_vulns_to_reattack(
        last_status=VulnerabilityStateStatus.OPEN,
        new_status=VulnerabilityStateStatus.OPEN,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    closed_vulns_ids: Set[str] = _get_vulns_ids(closed_vulns_to_reattack)
    open_vulns_ids: Set[str] = _get_vulns_ids(open_vulns_to_reattack)

    coroutines: List[Awaitable[bool]] = [
        _add_vulnerability_to_dynamo(
            vulnerability=vuln_in_db,
            to_update=vuln_to_add,
            finding_policy=finding_policy,
        )
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        # Exclude vulns that we'll verify as those state updates
        # are handled in the verification function
        if not vuln_in_db
        or (
            vuln_in_db.id not in closed_vulns_ids
            and vuln_in_db.id not in open_vulns_ids
        )
    ]

    if closed_vulns_to_reattack:
        coroutines.append(
            findings_domain.verify_vulnerabilities(
                context=context,
                finding_id=finding_id,
                user_info=user_info,
                justification=(
                    "The vulnerability was re-attacked "
                    "and found to be closed"
                ),
                open_vulns_ids=[],
                closed_vulns_ids=list(closed_vulns_ids),
                vulns_to_close_from_file=closed_vulns_to_reattack,
            )
        )
    if open_vulns_to_reattack:
        coroutines.append(
            findings_domain.verify_vulnerabilities(
                context=context,
                finding_id=finding_id,
                user_info=user_info,
                justification=(
                    "The vulnerability was re-attacked "
                    "and found to be still open"
                ),
                open_vulns_ids=list(open_vulns_ids),
                closed_vulns_ids=[],
                vulns_to_close_from_file=[],
            )
        )

    return all(await collect(coroutines))


async def process_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> bool:
    """Process a file."""
    raw_content = await file_input.read()
    raw_content = cast(bytes, raw_content).decode()
    file_content = html.escape(raw_content, quote=False)
    await file_input.seek(0)
    vulnerabilities = yaml.safe_load(file_content)
    # FP: the generated filename is unpredictable
    file_url = f"/tmp/vulnerabilities-{uuid.uuid4()}-{finding_id}.yaml"  # NOSONAR # nosec # noqa: E501
    with open(file_url, "w", encoding="utf-8") as stream:
        yaml.safe_dump(vulnerabilities, stream)
    if validate_file_schema(file_url, info):
        return await _map_vulnerabilities_to_dynamo(
            context=info.context,
            vulns_data_from_file=vulnerabilities,
            group_name=group_name,
            finding_id=finding_id,
            finding_policy=finding_policy,
        )
    return False


def ungroup_vulnerability_specific(
    vuln_type: str, specific: str, data: Dict[str, str]
) -> List[Dict[str, str]]:
    """Add vulnerability auxiliar."""
    if vuln_type in ("lines", "ports"):
        specific_values = vulns_utils.ungroup_specific(specific)
    else:
        specific_values = [spec for spec in specific.split(",") if spec]
    if vuln_type == "ports" and not all(
        (0 <= int(i) <= 65535) for i in specific_values
    ):
        error_value = f'"values": "{specific}"'
        raise InvalidPort(expr=error_value)
    if not specific_values:
        raise InvalidSpecific()
    return [{**data, "specific": specific} for specific in specific_values]


async def upload_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> bool:
    finding_loader = info.context.loaders.finding
    finding: Finding = await finding_loader.load(finding_id)
    if not operation_can_be_executed(info.context, finding.title):
        raise MachineCanNotOperate()

    mib = 1048576
    success = False
    if await files_utils.get_file_size(file_input) < 1 * mib:
        success = await process_file(
            info,
            file_input,
            finding_id,
            finding_policy,
            group_name,
        )
    else:
        raise InvalidFileSize()
    return success


def validate_file_schema(file_url: str, info: GraphQLResolveInfo) -> bool:
    """Validate if a file has the correct schema."""
    schema_dir = os.path.dirname(os.path.abspath(__file__))
    schema_dir = os.path.join(schema_dir, "vuln_template.yaml")
    core = Core(source_file=file_url, schema_files=[schema_dir])
    is_valid = False
    try:
        core.validate(raise_exception=True)
        is_valid = True
    except SchemaError as ex:
        lines_of_exceptions = core.errors
        errors_values = [
            getattr(x, "pattern", "")
            for x in lines_of_exceptions
            if not hasattr(x, "key")
        ]
        errors_keys = [x for x in lines_of_exceptions if hasattr(x, "key")]
        errors_values_formated = [json.dumps(x) for x in errors_values]
        errors_keys_formated = [f'"{x.key}"' for x in errors_keys]
        errors_keys_joined = ",".join(errors_keys_formated)
        errors_values_joined = ",".join(errors_values_formated)
        error_value = (
            f'"values": [{errors_values_joined}], '
            f'"keys": [{errors_keys_joined}]'
        )
        logs_utils.cloudwatch_log(
            info.context,
            "Error: An error occurred validating vulnerabilities file",
        )
        raise InvalidSchema(expr=error_value) from ex
    except CoreError as ex:
        raise InvalidSchema() from ex
    finally:
        os.unlink(file_url)
    return is_valid
