from aioextensions import (
    collect,
)
from back.src.machine.availability import (
    operation_can_be_executed,
)
from custom_exceptions import (
    ExpectedEscaperField,
    ExpectedPathToStartWithRepo,
    ExpectedVulnToHaveNickname,
    InvalidCannotModifyNicknameWhenClosing,
    InvalidFileSize,
    InvalidNewVulnState,
    InvalidPath,
    InvalidPort,
    InvalidSchema,
    InvalidSpecific,
    InvalidVulnsNumber,
    MachineCanNotOperate,
    RootNotFound,
)
from custom_types import (
    Finding as FindingType,
    User as UserType,
    Vulnerability as VulnerabilityType,
)
from dynamodb.types import (
    OrgFindingPolicyItem,
)
from findings import (
    domain as findings_domain,
)
from graphql.type.definition import (
    GraphQLResolveInfo,
)
import html
from itertools import (
    chain,
)
import json
from newutils import (
    datetime as datetime_utils,
    files as files_utils,
    logs as logs_utils,
    requests as requests_utils,
    token as token_utils,
    vulnerabilities as vulns_utils,
)
from operator import (
    itemgetter,
)
import os
from pykwalify.core import (
    Core,
)
from pykwalify.errors import (
    CoreError,
    SchemaError,
)
from starlette.datastructures import (
    UploadFile,
)
from typing import (
    Awaitable,
    cast,
    Dict,
    List,
    Optional,
    Set,
)
import uuid
from vulnerabilities import (
    dal as vulns_dal,
    domain as vulns_domain,
)
from vulnerabilities.domain.utils import (
    get_hash_from_dict,
)
from vulnerabilities.domain.validations import (
    validate_stream,
)
import yaml  # type: ignore


def get_treatment_new_vuln(
    *, current_day: str, finding_policy: Optional[OrgFindingPolicyItem]
) -> List[Dict[str, str]]:
    if finding_policy and finding_policy.state.status == "APPROVED":
        return vulns_utils.get_treatment_from_org_finding_policy(
            current_day=current_day,
            user_email=finding_policy.state.modified_by,
        )

    return [{"date": current_day, "treatment": "NEW"}]


async def add_vuln_to_dynamo(
    *,
    item: Dict[str, str],
    specific: str,
    finding_id: str,
    info: GraphQLResolveInfo,
    vulnerability: Optional[Dict[str, FindingType]],
    finding_policy: Optional[OrgFindingPolicyItem],
) -> bool:
    """Add or update vulnerability in dynamo"""
    historic_state = []
    response = False
    current_day = datetime_utils.get_now_as_str()
    user_data = cast(UserType, await token_utils.get_jwt_content(info.context))
    email = str(user_data["user_email"])
    if vulnerability:
        return await vulns_domain.update_vuln_state(
            info=info,
            vulnerability=vulnerability,
            item=item,
            finding_id=finding_id,
            current_day=current_day,
            finding_policy=finding_policy,
        )

    data: Dict[str, FindingType] = {}

    # Prepare new entry in historic_state
    new_state: Dict[str, str] = {
        "date": current_day,
    }
    if item.get("source"):
        new_state["source"] = item["source"]
    else:
        new_state["source"] = requests_utils.get_source(info.context)
    if item.get("escaper"):
        new_state["analyst"] = item["escaper"]
    else:
        new_state["analyst"] = email

    data["historic_treatment"] = get_treatment_new_vuln(
        current_day=current_day,
        finding_policy=finding_policy,
    )
    data["vuln_type"] = item.get("vuln_type", "")
    data["where"] = item.get("where", "")
    data["specific"] = specific
    data["finding_id"] = finding_id
    if "repo_nickname" in item:
        data["repo_nickname"] = item["repo_nickname"]
    if "stream" in item:
        data["stream"] = item["stream"]
    if "commit_hash" in item:
        data["commit_hash"] = item["commit_hash"]
    data["UUID"] = str(uuid.uuid4())
    if item.get("state"):
        if item.get("state") != "open":
            raise InvalidNewVulnState.new()
        new_state["state"] = item["state"]
        historic_state.append(new_state)
        data["historic_state"] = historic_state
        response = await vulns_dal.create(data)
    else:
        logs_utils.cloudwatch_log(
            info.context,
            "Security: Attempted to add vulnerability without state",
        )
    return response


def _deduplicate_vulns(vulns: List[Dict[str, str]]) -> List[Dict[str, str]]:
    # In case there are repeated vulns, only the latest will be taken into
    # account
    return list({get_hash_from_dict(vuln): vuln for vuln in vulns}.values())


async def get_vulns_to_add(
    vulnerabilities: Dict[str, List[VulnerabilityType]],
) -> List[Dict[str, str]]:
    return _deduplicate_vulns(
        list(
            chain.from_iterable(
                map_vulnerability_type(
                    vuln,
                    vuln_type,
                )
                for vuln_type in ["inputs", "lines", "ports"]
                for vuln in vulnerabilities.get(vuln_type, [])
            )
        )
    )


def map_vulnerability_type(  # noqa: MC0001
    item: VulnerabilityType,
    vuln_type: str,
) -> List[Dict[str, str]]:
    """Map fields according to vuln type"""
    response = []
    where_headers = {
        "inputs": {"where": "url", "specific": "field"},
        "lines": {"where": "path", "specific": "line"},
        "ports": {"where": "host", "specific": "port"},
    }

    data: Dict[str, str] = {
        "state": item["state"],
        "vuln_type": vuln_type,
        "where": item[where_headers[vuln_type]["where"]],
    }

    # Validate source and escaper field
    if item.get("source"):
        data["source"] = item["source"]
        if item["source"] == "escape" and "escaper" not in item:
            raise ExpectedEscaperField.new()
        data["escaper"] = item.get("escaper", None)

    specific: str = item[where_headers[vuln_type]["specific"]]
    if "repo_nickname" in item:
        data["repo_nickname"] = item["repo_nickname"]
    else:
        raise ExpectedVulnToHaveNickname.new()

    if vuln_type == "lines":
        # Propagate the commit_hash
        data["commit_hash"] = item["commit_hash"]

        # Where must start with repo_nickname
        if not data["where"].startswith(data["repo_nickname"]):
            raise ExpectedPathToStartWithRepo.new()

        # Use Unix-like paths
        if data["where"].find("\\") >= 0:
            path = data["where"].replace("\\", "\\\\")
            raise InvalidPath(expr=f'"values": "{path}"')

    elif vuln_type == "inputs":
        # Propagate the stream
        validate_stream(data["where"], item["stream"])
        data["stream"] = item["stream"]

    elif vuln_type == "ports":
        # Nothing custom for now
        pass

    if vulns_utils.is_range(specific) or vulns_utils.is_sequence(specific):
        response.extend(
            ungroup_vulnerability_specific(vuln_type, specific, data)
        )
    else:
        if vuln_type == "ports" and not 0 <= int(specific) <= 65535:
            raise InvalidPort(expr=f'"values": "{specific}"')
        response.append({**data, "specific": specific})

    return response


def _get_vulns_to_reattack(
    last_state: str,
    new_state: str,
    vulns_in_db: List[Dict[str, FindingType]],
    vulns_to_add: List[Dict[str, str]],
) -> List[Dict[str, str]]:
    return sorted(
        [
            {**vuln_to_add, "UUID": vuln_in_db["UUID"]}
            for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add)
            if (
                vulns_utils.is_reattack_requested(vuln_in_db)
                and vulns_utils.get_last_status(vuln_in_db) == last_state
                and vuln_to_add.get("state") == new_state
            )
        ],
        key=itemgetter("UUID"),
    )


async def _validate_nicknames(
    group: str,
    info: GraphQLResolveInfo,
    vulns_in_db: List[Dict[str, FindingType]],
    vulns_to_add: List[Dict[str, str]],
) -> None:
    nicknames: Set[str] = {
        root.state.nickname
        for root in await info.context.loaders.group_roots.load(group)
        if root.state.status == "ACTIVE"
    }

    for vuln_in_db, vuln_to_add in zip(vulns_in_db, vulns_to_add):

        # You can only report open vulns in an existing root
        if (
            vuln_to_add["state"] == "open"
            and vuln_to_add["repo_nickname"] not in nicknames
        ):
            raise RootNotFound()

        # When closing you can set the nickname if it does not exist
        # otherwise it must be equal to the existent nickname
        if vuln_to_add["state"] == "closed":
            if vuln_in_db.get("repo_nickname"):
                if vuln_in_db["repo_nickname"] != vuln_to_add["repo_nickname"]:
                    raise InvalidCannotModifyNicknameWhenClosing.new()
            else:
                if vuln_to_add["repo_nickname"] not in nicknames:
                    raise RootNotFound()


def _get_vulns_uuids(vulns: List[Dict[str, str]]) -> Set[str]:
    return set(map(itemgetter("UUID"), vulns))


def _sort_vulns_for_comparison(
    vulns: List[Dict[str, FindingType]]
) -> List[Dict[str, FindingType]]:
    """Sort vulns, opened first, nickname last"""
    # Open vulns have priority so they can be closed
    # No-Nickname vulns have priority so they can be set their nickname
    return sorted(
        vulns,
        key=lambda vuln: (
            # Open ones first
            vulns_utils.get_last_status(vuln) != "open",
            # Nickname last
            bool(vuln.get("repo_nickname")),
        ),
    )


async def map_vulns_to_dynamo(
    info: GraphQLResolveInfo,
    vulnerabilities: Dict[str, List[VulnerabilityType]],
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> bool:
    """Map vulnerabilities and send it to dynamo"""
    # Vulns uploaded by the user
    vulns_to_add = await get_vulns_to_add(vulnerabilities)

    # Avoid DoS
    if len(vulns_to_add) > 100:
        raise InvalidVulnsNumber()

    # Vulns as they appear in the DB
    finding_vulns = await vulns_dal.get_by_finding(str(finding_id))
    sorted_vulns = _sort_vulns_for_comparison(finding_vulns)
    vulns_in_db: List[Dict[str, FindingType]] = [
        next(
            iter(
                [
                    vuln
                    for vuln in sorted_vulns
                    if get_hash_from_dict(vuln_to_add)
                    == get_hash_from_dict(vuln)
                    and (
                        vulns_utils.get_last_status(vuln) == "open"
                        or (
                            vulns_utils.get_last_status(vuln) == "closed"
                            and vuln_to_add["state"] == "closed"
                        )
                    )
                ]
            ),
            {},
        )
        for vuln_to_add in vulns_to_add
    ]

    # Validate vulnerability roots
    await _validate_nicknames(
        group=group_name,
        info=info,
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )

    # Vulnerabilities that have a requested reattack are managed differently
    # They need to have their historic_verification modified so we avoid
    # open vulns being closed in this batch from being pending to reattack
    closed_vulns_to_reattack: List[Dict[str, str]] = _get_vulns_to_reattack(
        last_state="open",
        new_state="closed",
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    open_vulns_to_reattack: List[Dict[str, str]] = _get_vulns_to_reattack(
        last_state="open",
        new_state="open",
        vulns_in_db=vulns_in_db,
        vulns_to_add=vulns_to_add,
    )
    closed_vulns_uuids: Set[str] = _get_vulns_uuids(closed_vulns_to_reattack)
    open_vulns_uuids: Set[str] = _get_vulns_uuids(open_vulns_to_reattack)

    coroutines: List[Awaitable[bool]] = [
        add_vuln_to_dynamo(
            item=vuln_to_add,
            specific=vuln_to_add["specific"],
            finding_id=finding_id,
            info=info,
            vulnerability=vuln_in_db,
            finding_policy=finding_policy,
        )
        for vuln_to_add, vuln_in_db in zip(vulns_to_add, vulns_in_db)
        # Exclude vulns that we'll verify as those state updates
        # are handled in the verification function
        if vuln_in_db.get("UUID") not in closed_vulns_uuids
        and vuln_in_db.get("UUID") not in open_vulns_uuids
    ]

    user_data = cast(UserType, await token_utils.get_jwt_content(info.context))

    if closed_vulns_to_reattack:
        coroutines.append(
            findings_domain.verify_vulnerabilities(
                info=info,
                finding_id=finding_id,
                user_info=user_data,
                parameters={
                    "justification": (
                        "The vulnerability was re-attacked "
                        "and found to be closed"
                    ),
                    "closed_vulns": list(closed_vulns_uuids),
                },
                vulns_to_close_from_file=closed_vulns_to_reattack,
            )
        )
    if open_vulns_to_reattack:
        coroutines.append(
            findings_domain.verify_vulnerabilities(
                info=info,
                finding_id=finding_id,
                user_info=user_data,
                parameters={
                    "justification": (
                        "The vulnerability was re-attacked "
                        "and found to be still open"
                    ),
                    "open_vulns": list(open_vulns_uuids),
                },
                vulns_to_close_from_file=open_vulns_to_reattack,
            )
        )

    return all(await collect(coroutines))


async def process_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> bool:
    """Process a file."""
    success = False
    raw_content = await file_input.read()
    raw_content = cast(bytes, raw_content).decode()
    file_content = html.escape(raw_content, quote=False)
    await file_input.seek(0)
    vulnerabilities = yaml.safe_load(file_content)
    # FP: the generated filename is unpredictable
    file_url = f"/tmp/vulnerabilities-{uuid.uuid4()}-{finding_id}.yaml"  # NOSONAR # nosec # noqa: E501
    with open(file_url, "w", encoding="utf-8") as stream:
        yaml.safe_dump(vulnerabilities, stream)
    if validate_file_schema(file_url, info):
        success = await map_vulns_to_dynamo(
            info,
            vulnerabilities,
            finding_id,
            finding_policy,
            group_name=group_name,
        )
    else:
        success = False
    return success


def ungroup_vulnerability_specific(
    vuln: str, specific: str, data: Dict[str, str]
) -> List[Dict[str, str]]:
    """Add vulnerability auxiliar."""
    if vuln in ("lines", "ports"):
        specific_values = vulns_utils.ungroup_specific(specific)
    else:
        specific_values = [spec for spec in specific.split(",") if spec]
    if vuln == "ports" and not all(
        (0 <= int(i) <= 65535) for i in specific_values
    ):
        error_value = f'"values": "{specific}"'
        raise InvalidPort(expr=error_value)
    if not specific_values:
        raise InvalidSpecific()
    return [{**data, "specific": specific} for specific in specific_values]


async def upload_file(
    info: GraphQLResolveInfo,
    file_input: UploadFile,
    finding_id: str,
    finding_policy: Optional[OrgFindingPolicyItem],
    group_name: str,
) -> bool:
    finding_loader = info.context.loaders.finding
    finding: FindingType = await finding_loader.load(finding_id)
    if not operation_can_be_executed(info.context, finding.title):
        raise MachineCanNotOperate()

    mib = 1048576
    success = False
    if await files_utils.get_file_size(file_input) < 1 * mib:
        success = await process_file(
            info,
            file_input,
            finding_id,
            finding_policy,
            group_name,
        )
    else:
        raise InvalidFileSize()
    return success


def validate_file_schema(file_url: str, info: GraphQLResolveInfo) -> bool:
    """Validate if a file has the correct schema."""
    schema_dir = os.path.dirname(os.path.abspath(__file__))
    schema_dir = os.path.join(schema_dir, "vuln_template.yaml")
    core = Core(source_file=file_url, schema_files=[schema_dir])
    is_valid = False
    try:
        core.validate(raise_exception=True)
        is_valid = True
    except SchemaError as ex:
        lines_of_exceptions = core.errors
        errors_values = [
            getattr(x, "pattern", "")
            for x in lines_of_exceptions
            if not hasattr(x, "key")
        ]
        errors_keys = [x for x in lines_of_exceptions if hasattr(x, "key")]
        errors_values_formated = [json.dumps(x) for x in errors_values]
        errors_keys_formated = [f'"{x.key}"' for x in errors_keys]
        errors_keys_joined = ",".join(errors_keys_formated)
        errors_values_joined = ",".join(errors_values_formated)
        error_value = (
            f'"values": [{errors_values_joined}], '
            f'"keys": [{errors_keys_joined}]'
        )
        logs_utils.cloudwatch_log(
            info.context,
            "Error: An error occurred validating vulnerabilities file",
        )
        raise InvalidSchema(expr=error_value) from ex
    except CoreError as ex:
        raise InvalidSchema() from ex
    finally:
        os.unlink(file_url)
    return is_valid
